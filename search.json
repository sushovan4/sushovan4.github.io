[
  {
    "objectID": "solutions.html",
    "href": "solutions.html",
    "title": "Solution Manuals",
    "section": "",
    "text": "name\n\n\nFirst Item\n\n\n\n\nhref\n\n\nhttps://www.quarto.org\n\n\n\n\ncustom-field\n\n\nA custom value\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\n\n\nSecond Item\n\n\n\n\nhref\n\n\nhttps://www.rstudio.org\n\n\n\n\ncustom-field\n\n\nA second custom value\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/data-science/qq.html",
    "href": "tutorials/data-science/qq.html",
    "title": "Quantile-Quantile Plots",
    "section": "",
    "text": "Quantile-Quantile plot (also known as Q-Q plot) is an extremely useful visual tool for exploratory data analysis (EDA). A Q-Q plot is not particularly a summary of data, rather an informal assessment of goodness of fit to discern the disparity of two distributions. Quantiles from one distribution (usually from data) is plotted against those of another distribution (usually a theoretical, known model). For more examples and discussions, see [1]."
  },
  {
    "objectID": "tutorials/data-science/qq.html#theoretical-quantiles",
    "href": "tutorials/data-science/qq.html#theoretical-quantiles",
    "title": "Quantile-Quantile Plots",
    "section": "Theoretical Quantiles",
    "text": "Theoretical Quantiles\n\nDefinition 1 (Theoretical Quantiles) For any p\\in[0,1], a pth quantile of a random variable X is defined to be that value x_p\\in\\mathbb R such that \\mathbb P(X\\leq x_p)=p.\n\nIn other words, the probability that X realizes a value not greater than a pth quantile is p. For p=\\frac{1}{2}, x_p is commonly known as median of X. If F(x) denotes the CDF of X, one notes that x_p=F^{-1}(p), provided the CDF F(x) is invertible1 near x_p.\n\n\n\n\n\n\nQuantiles are not unique\n\n\n\nIn general, quantiles are not unique. Easy examples can be found when X is discrete. For an example on the continuous side, take X\\sim\\mathrm{unif}([0,1]) to see that any number not less than 1 is a pth quantile for p=1. See the CDF of X below to convince yourself.\n\n\nPlot.plot({\ngrid: true,\ny: {  ticks: 5 },\nx: { ticks: 3 },\nstyle: { fontSize: '1.2rem' },\nmarks: [\nPlot.line([[-1, 0],[0, 0]], { stroke: 'steelblue', strokeWidth: 3 }),\nPlot.line([[0, 1],[1, 1]], { stroke: 'steelblue', strokeWidth: 3 }),\nPlot.line([[1, 0],[2, 0]], { stroke: 'steelblue', strokeWidth: 3 }),\n]\n})\nPlot.line([[-1, 0],[0, 0],[1, 1], [2, 1]], { stroke: 'steelblue', strokeWidth: 3 }).plot({\ngrid: true,\ny: {  ticks: 5 },\nx: { ticks: 3 },\nstyle: { fontSize: '1.4rem' }\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) PDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) CDF\n\n\n\n\n\n\n\nFigure 1: The density (left) and cumulative distribution (right) functions of uniform [0,1] are shown by the blue lines. The CDF is only invertible on the support.\n\n\n\nMoving forward, we assume that the X is a continuous random variables and that its CDF F is a strictly increasing, continuous function, at least on an interval of the real line. As a consequence, the CDF is invertible everywhere and the pth quantile x_p=F^{-1}(p) is uniquely defined. Examples of such distributions include the exponential, \\chi^2- and F-distribution on (0,\\infty), normal and Student’s t-distribution on \\mathbb R, the Beta distribution on (0,1), etc.\n\nExercise 1 Find a continuous distribution with the expected value 0 and whose CDF is only intertible on a bounded interval of the real line."
  },
  {
    "objectID": "tutorials/data-science/qq.html#observed-quantiles",
    "href": "tutorials/data-science/qq.html#observed-quantiles",
    "title": "Quantile-Quantile Plots",
    "section": "Observed Quantiles",
    "text": "Observed Quantiles\nWhile the quantiles of a probability distribution can be concretely defined (Definition 1), there have been quite a few conventions for the assignment of quantiles for a batch of observations or a dataset. Although, for a large sample they make little to no difference for a descriptive analysis. We use the following convention:\nFor a random sample X_1, X_2, \\ldots, X_n of size n, the order statistics are denoted by X_{(1)}\\leq X_{(2)}\\leq\\ldots\\leq X_{(n)}. And, the k/(n+1) quantile of data is assigned to X_{(k)}, the kth-order statistic."
  },
  {
    "objectID": "tutorials/data-science/qq.html#observed-vs-theoretical-quantiles",
    "href": "tutorials/data-science/qq.html#observed-vs-theoretical-quantiles",
    "title": "Quantile-Quantile Plots",
    "section": "Observed vs Theoretical Quantiles",
    "text": "Observed vs Theoretical Quantiles\nLet us consider a sample of n from a uniform distribution from [0,1]. As proved in\n\nTesting Uniform Random Generator\n\nviewof n = Inputs.range([30,100],{ step: 1, label: 'sample size' })\nPlot.plot({\n  style: {  },\n  grid: true,\n  x: { label: `uniform quantiles →`, line: true },\n  y: { label: `↑ observed quantiles`, line: true },\n  marks: [\n    Plot.link({length: 1}, {\n      x1: 0,\n      x2: 1,\n      y1: 0,\n      y2: 1,\n    }),\n    Plot.dot({length: n}, {\n      x: d3.range(n).map(i =&gt; (i+1)/(n+1)),\n      y: d3.sort( Array.from({length: n}, d3.randomUniform()) )\n    }),\n  ]\n})"
  },
  {
    "objectID": "tutorials/data-science/qq.html#two-observed-batches",
    "href": "tutorials/data-science/qq.html#two-observed-batches",
    "title": "Quantile-Quantile Plots",
    "section": "Two Observed Batches",
    "text": "Two Observed Batches\n\ndata = await FileAttachment('Bjerkdahl.csv').csv({ typed: true });\ndataA = data.filter( d =&gt; d.Treatment === 'III' ).map( d =&gt; d.X ).sort( (a, b) =&gt; a - b);\ndataB = data.filter( d =&gt; d.Treatment === 'V' ).map( d =&gt; d.X ).sort( (a, b) =&gt; a - b);\nPlot.plot({\n  x: {\n    label: 'Quantiles of Group A'\n  },\n  y: {\n    label: 'Quantiles of Group B'\n  },\n  width: 500,\n  marks: [\n    Plot.dot(dataB.map( (q, i) =&gt; [ q, dataA[i] ] ), { fill: 'steelblue' }),\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    Plot.line([[0,0], [500, 500]], { stroke: 'gray' })\n  ], \n})"
  },
  {
    "objectID": "tutorials/data-science/qq.html#footnotes",
    "href": "tutorials/data-science/qq.html#footnotes",
    "title": "Quantile-Quantile Plots",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA function f:A\\to B is called invertible near a point x_0\\in A if there is an interval I containing the point x_0 such that f is a bijective map when restricted on I.↩︎"
  },
  {
    "objectID": "tutorials/data-science/regression/index.html",
    "href": "tutorials/data-science/regression/index.html",
    "title": "Large-Sample Regression Theory",
    "section": "",
    "text": "We write a k-vector (of scalars) as a row \n{\\boldsymbol{x}}=\n\\begin{bmatrix}\nx_1 &\nx_2 &\n\\ldots &\nx_k\n\\end{bmatrix}.\n The transpose of {\\boldsymbol{x}} as \n{\\boldsymbol{x}}^T=\n\\begin{bmatrix}\nx_1 \\\\ x_2\\\\ \\vdots \\\\ x_k\n\\end{bmatrix}.\n\nWe use uppercase letters X,Y,Z,\\ldots to denote random variables. Random vectors are denoted by bold uppercase letters {\\boldsymbol{X}},{\\boldsymbol{Y}},{\\boldsymbol{Z}},\\ldots, and written as a row vector. For example, \n{\\boldsymbol{X}}=\n\\begin{bmatrix}\nX_{[1]} &\nX_{[2]} &\n\\ldots &\nX_{[k]}\n\\end{bmatrix}.\n\nIn order to distinguish random matrices from vectors, a random matrix is denoted by {\\mathbb{X}}.\nThe expectation of {\\boldsymbol{X}} is defined as \n{\\mathbb{E}\\left[ {\\boldsymbol{X}} \\right]}=\n\\begin{bmatrix}\n{\\mathbb{E}\\left[ X_{[1]} \\right]} &\n{\\mathbb{E}\\left[ X_{[2]} \\right]} &\n\\ldots &\n{\\mathbb{E}\\left[ X_{[k]} \\right]}\n\\end{bmatrix}.\n The k\\times k covariance matrix of {\\boldsymbol{X}} is defined as \n\\begin{aligned}\n{\\mathbb{V}\\left[ {\\boldsymbol{X}} \\right]} &={\\mathbb{E}\\left[ ({\\boldsymbol{X}}-{\\mathbb{E}\\left[ {\\boldsymbol{X}} \\right]})^T({\\boldsymbol{X}}-{\\mathbb{E}\\left[ {\\boldsymbol{X}} \\right]}) \\right]} \\\\\n&=\\begin{bmatrix}\n\\sigma_1^2 & \\sigma_{12} & \\ldots & \\sigma_{1k} \\\\\n\\sigma_{21} & \\sigma_{2}^2 & \\ldots & \\sigma_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{k1} & \\sigma_{k2}^2 & \\ldots & \\sigma_{k}^2 \\\\\n\\end{bmatrix}_{k\\times k}\n\\end{aligned}\n\nwhere \\sigma_j={\\mathbb{V}\\left[ X_{[j]} \\right]} and \\sigma_{ij}={\\text{Cov}\\left[ X_{[i]},X_{[j]} \\right]} for i,j=1,2,\\ldots,k and i\\neq j.\n\nTheorem 1 (Linearity of Exectation) Let {\\mathbb{A}}_{l\\times k},{\\mathbb{B}}_{m\\times l} be fixed matrices and {\\boldsymbol{c}} a fixed vector of size l. If {\\boldsymbol{X}} and {\\boldsymbol{Y}} are random vectors of size k and m, respectively, such that {\\mathbb{E}\\left[ X \\right]}&lt;\\infty,{\\mathbb{E}\\left[ Y \\right]}&lt;\\infty, then \n{\\mathbb{E}\\left[ {\\mathbb{A}}{\\boldsymbol{X}}+{\\boldsymbol{Y}}{\\mathbb{B}}+{\\boldsymbol{c}} \\right]}={\\mathbb{A}}{\\mathbb{E}\\left[ {\\boldsymbol{X}} \\right]}+{\\mathbb{E}\\left[ {\\boldsymbol{Y}} \\right]}{\\mathbb{B}}+{\\boldsymbol{c}}."
  },
  {
    "objectID": "tutorials/data-science/regression/index.html#cef-bivariate",
    "href": "tutorials/data-science/regression/index.html#cef-bivariate",
    "title": "Large-Sample Regression Theory",
    "section": "CEF Bivariate",
    "text": "CEF Bivariate"
  },
  {
    "objectID": "tutorials/data-science/regression/index.html#optional-cef-multivariate",
    "href": "tutorials/data-science/regression/index.html#optional-cef-multivariate",
    "title": "Large-Sample Regression Theory",
    "section": "(Optional) CEF Multivariate",
    "text": "(Optional) CEF Multivariate\n\nTheorem 2 (Characterization of CEF) If  {\\mathbb{E}\\left[ Y^2 \\right]}&lt;\\infty and {\\boldsymbol{X}} is a random vector such that Y=m({\\boldsymbol{X}})+e, then the following statements are equivalent:\n1. m({\\boldsymbol{X}})={\\mathbb{E}\\left[ Y|{\\boldsymbol{X}} \\right]}, the CEF of Y given {\\boldsymbol{X}}\n2. {\\mathbb{E}\\left[ e|{\\boldsymbol{X}} \\right]}=0"
  },
  {
    "objectID": "tutorials/data-science/regression/index.html#best-linear-predictor",
    "href": "tutorials/data-science/regression/index.html#best-linear-predictor",
    "title": "Large-Sample Regression Theory",
    "section": "Best Linear Predictor",
    "text": "Best Linear Predictor\nLet Y be a random variable and {\\boldsymbol{X}} be a random vector of k variables. We denote the best linear predictor of Y given {\\boldsymbol{X}} by \\mathscr{P}[Y|{\\boldsymbol{X}}]. It’s also called the linear projection of Y on {\\boldsymbol{X}}.\n\nTheorem 3 (Best Linear Predictor) Under the following assumptions\n\n{\\mathbb{E}\\left[ Y^2 \\right]}&lt;\\infty\n{\\mathbb{E}\\left[ ||\\bf{X}||^2 \\right]}&lt;\\infty\n{\\mathbb{Q}}_{\\bf{XX}}\\stackrel{\\text{def}}{=}{\\mathbb{E}\\left[ {\\boldsymbol{X}}^T{\\boldsymbol{X}} \\right]} is positive-definite\n\nthe best linear predictor exists uniquely, and has the form \n\\mathscr{P}[Y|{\\boldsymbol{X}}]={\\boldsymbol{X}}{\\boldsymbol{\\beta}},\n where {\\boldsymbol{\\beta}}=\\left({\\mathbb{E}\\left[ {\\boldsymbol{X}}^T{\\boldsymbol{X}} \\right]}\\right)^{-1}{\\mathbb{E}}[{\\boldsymbol{X}}^TY] is a column vector.\n\nIn the following theorem, we show that the BLP error is uncorrelated to the explanatory variables.\n\nTheorem 4 (Best Linear Predictor Error) If the BLP exists, the linear projection error \\varepsilon=Y-\\mathscr{P}[Y|{\\boldsymbol{X}}] follows the following properties:\n\n{\\mathbb{E}}[{\\boldsymbol{X}}^T\\varepsilon]={\\boldsymbol{0}}\nmoreover, {\\mathbb{E}}[\\varepsilon]=0 if {\\boldsymbol{X}}=\\begin{bmatrix}1 & X_{[1]} & \\ldots & X_{[k]} \\end{bmatrix} contains a constant."
  },
  {
    "objectID": "tutorials/data-science/regression/index.html#consistency-of-ols-estimators",
    "href": "tutorials/data-science/regression/index.html#consistency-of-ols-estimators",
    "title": "Large-Sample Regression Theory",
    "section": "Consistency of OLS Estimators",
    "text": "Consistency of OLS Estimators"
  },
  {
    "objectID": "tutorials/data-science/regression/index.html#asymptotic-normality",
    "href": "tutorials/data-science/regression/index.html#asymptotic-normality",
    "title": "Large-Sample Regression Theory",
    "section": "Asymptotic Normality",
    "text": "Asymptotic Normality\nWe start by revealing an alternative expression for the OLS estimators \\widehat{{\\boldsymbol{\\beta}}} using matrix notation.\n\n\\begin{aligned}\n\\widehat{{\\boldsymbol{\\beta}}}\n&=\\left[{\\mathbb{X}}^T{\\mathbb{X}}\\right]^{-1}{\\mathbb{X}}^T{\\boldsymbol{Y}} \\\\\n&=\\left[{\\mathbb{X}}^T{\\mathbb{X}}\\right]^{-1}{\\mathbb{X}}^T({\\mathbb{X}}{\\boldsymbol{\\beta}}+{\\boldsymbol{\\varepsilon}}) \\\\\n&=\\left[{\\mathbb{X}}^T{\\mathbb{X}}\\right]^{-1}({\\mathbb{X}}^T{\\mathbb{X}}){\\boldsymbol{\\beta}}+\n\\left[{\\mathbb{X}}^T{\\mathbb{X}}\\right]^{-1}{\\mathbb{X}}^T{\\boldsymbol{\\varepsilon}} \\\\\n&={\\boldsymbol{\\beta}} + \\left[{\\mathbb{X}}^T{\\mathbb{X}}\\right]^{-1}{\\mathbb{X}}^T{\\boldsymbol{\\varepsilon}}\n\\end{aligned}\n\nSo, \n\\widehat{{\\boldsymbol{\\beta}}}-{\\boldsymbol{\\beta}} = \\left[{\\mathbb{X}}^T{\\mathbb{X}}\\right]^{-1}{\\mathbb{X}}^T{\\boldsymbol{\\varepsilon}}\n\\tag{1}\nWe can then multiply by \\sqrt{n} both sides of Equation 1 to get \n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{{\\boldsymbol{\\beta}}}-{\\boldsymbol{\\beta}}\\right)\n&=\\left( \\frac{1}{n}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^T{\\boldsymbol{X}}_i \\right)^{-1}\n\\left( \\frac{1}{\\sqrt{n}}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^T\\varepsilon_i \\right) \\\\\n&=\\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{XX}}}^{-1}\n\\left( \\frac{1}{\\sqrt{n}}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^T\\varepsilon_i \\right)\n\\end{aligned}\n From the consistency of OLS estimators, we already have  \\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{XX}}}\\xrightarrow[p]{\\quad\\quad}{\\mathbb{Q}}_{{\\boldsymbol{XX}}} Our aim now is to understand the distribution of the stochastic term (the second term) in the above expression.\nWe first note (from i.i.d. and Theorem 4) that \n{\\mathbb{E}\\left[ {\\boldsymbol{X}}_i^T\\varepsilon_i \\right]}={\\mathbb{E}\\left[ {\\boldsymbol{X}}^T\\varepsilon \\right]}={\\boldsymbol{0}}.\n Let us compute the covariance matrix of {\\boldsymbol{X}}_i\\varepsilon_i. Since the expectation vector is zero, we have \n{\\mathbb{V}}[{\\boldsymbol{X}}_i^T\\varepsilon_i]={\\mathbb{E}\\left[ {\\boldsymbol{X}}_i^T\\varepsilon_i\\left({\\boldsymbol{X}}_i^T\\varepsilon_i\\right)^T \\right]}={\\mathbb{E}\\left[ {\\boldsymbol{X}}^T{\\boldsymbol{X}}\\varepsilon^2 \\right]}\\stackrel{\\text{def}}{=}{\\mathbb{A}}.\n As any function of \\{(Y_i,{\\boldsymbol{X}}_i)\\}’s are independent, \\{{\\boldsymbol{X}}_i\\varepsilon_i\\}’s are independent. By the (multivariate) Central Limit Theorem, as n\\to\\infty \n\\frac{1}{\\sqrt{n}}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^T\\varepsilon_i\n\\xrightarrow[d]{\\quad\\quad}\\mathcal{N}({\\boldsymbol{0}},{\\mathbb{A}}).\n There is a small technicality here, we must have {\\mathbb{A}}&lt;\\infty. This can be imposed by a stronger regularity condition on the moments, e.g., {\\mathbb{E}\\left[ Y^4 \\right]},{\\mathbb{E}\\left[ ||{\\boldsymbol{X}}||^4 \\right]}&lt;\\infty. Putting everything together, we conclude \n\\sqrt{n}(\\widehat{{\\boldsymbol{\\beta}}}-{\\boldsymbol{\\beta}})\\xrightarrow[d]{\\quad\\quad}\n{\\mathbb{Q}}_{{\\boldsymbol{XX}}}^{-1}\\mathcal{N}({\\boldsymbol{0}},{\\mathbb{A}})\n=\\mathcal{N}\\left({\\boldsymbol{0}},\\left[{\\mathbb{Q}}_{{\\boldsymbol{XX}}}^{-1}\\right]^T{\\mathbb{A}}{\\mathbb{Q}}_{{\\boldsymbol{XX}}}^{-1}\\right)\n=\\mathcal{N}\\left({\\boldsymbol{0}},{\\mathbb{Q}}_{{\\boldsymbol{XX}}}^{-1}{\\mathbb{A}}{\\mathbb{Q}}_{{\\boldsymbol{XX}}}^{-1}\\right)\n\n\nTheorem 5 (Asymptotic Distribution of OLS Estimators) We assume the following:\n1. The observations \\{(Y_i,{\\boldsymbol{X}}_i)\\}_{i=1}^n are i.i.d from the joint distribution of (Y,{\\boldsymbol{X}})\n2. {\\mathbb{E}\\left[ Y^4 \\right]}&lt;\\infty\n3. {\\mathbb{E}\\left[ ||{\\boldsymbol{X}}||^4 \\right]}&lt;\\infty\n4. {\\mathbb{Q}}_{{\\boldsymbol{XX}}}={\\mathbb{E}\\left[ {\\boldsymbol{X}}{\\boldsymbol{X}}' \\right]} is positive-definite. Under these assumptions, as n\\to\\infty \n\\sqrt{n}(\\widehat{{\\boldsymbol{\\beta}}}-{\\boldsymbol{\\beta}})\\xrightarrow[d]{\\quad\\quad}\n\\mathcal{N}\\left({\\boldsymbol{0}},{\\mathbb{V}}_{{\\boldsymbol{\\beta}}}\\right),\n where {\\mathbb{V}}_{{\\boldsymbol{\\beta}}}\\stackrel{\\text{def}}{=}{\\mathbb{Q}}_{{\\boldsymbol{XX}}}^{-1}{\\mathbb{A}}{\\mathbb{Q}}_{{\\boldsymbol{XX}}}^{-1} and {\\mathbb{Q}}_{{\\boldsymbol{XX}}}={\\mathbb{E}\\left[ {\\boldsymbol{X}}^T{\\boldsymbol{X}} \\right]}, {\\mathbb{A}}={\\mathbb{E}\\left[ {\\boldsymbol{X}}^T{\\boldsymbol{X}}\\varepsilon^2 \\right]}.\n\nThe covariance matrix {\\mathbb{V}}_{{\\boldsymbol{\\beta}}} is called the asymptotic variance matrix of \\widehat{{\\boldsymbol{\\beta}}}. The matrix is sometimes referred to as the sandwich form."
  },
  {
    "objectID": "tutorials/data-science/regression/index.html#covariance-matrix-estimation",
    "href": "tutorials/data-science/regression/index.html#covariance-matrix-estimation",
    "title": "Large-Sample Regression Theory",
    "section": "Covariance Matrix Estimation",
    "text": "Covariance Matrix Estimation\nWe now turn our attention to the estimation of the sandwich matrix using a finite sample.\n\nHeteroskedastic Variance\nTheorem 5 presented the asymptotic covariance matrix of \\sqrt{n}(\\widehat{{\\boldsymbol{\\beta}}}-{\\boldsymbol{\\beta}}) is {\\mathbb{V}}_{{\\boldsymbol{\\beta}}}\n={\\mathbb{Q}}_{{\\boldsymbol{XX}}}^{-1}{\\mathbb{A}}{\\mathbb{Q}}_{{\\boldsymbol{XX}}}^{-1}. Without imposing any homoskedasticity condition, we estimate {\\mathbb{V}}_{{\\boldsymbol{\\beta}}} using a plug-in estimator.\nWe have already seen that \\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{XX}}}=\\frac{1}{n}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^T{\\boldsymbol{X}}_i is a natural estimator for {\\mathbb{Q}}_{{\\boldsymbol{XX}}}. For {\\mathbb{A}}, we use the moment estimator \n\\widehat{{\\mathbb{A}}}=\\frac{1}{n}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^T{\\boldsymbol{X}}_ie_i^2,\n where e_i=(Y_i-{\\boldsymbol{X}}_i\\widehat{{\\boldsymbol{\\beta}}}) is the i-th residual. As it turns out, \\widehat{{\\mathbb{A}}} is a consistent estimator for {\\mathbb{A}}.\nAs a result, we get the following plug-in estimator for {\\mathbb{V}}_{{\\boldsymbol{\\beta}}}: \n\\widehat{{\\mathbb{V}}}_{{\\boldsymbol{\\beta}}}=\n\\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{XX}}}^{-1}\\widehat{{\\mathbb{A}}}\\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{XX}}}^{-1}\n The estimator is also consistent. For a proof, see Hensen 2013.\nAs a consequence, we can get the following estimator for the variance, {\\mathbb{V}}_{\\widehat{{\\boldsymbol{\\beta}}}}, of \\widehat{{\\boldsymbol{\\beta}}} in the heteroskedastic case. \n\\begin{aligned}\n\\widehat{{\\mathbb{V}}}\\left[\\widehat{{\\boldsymbol{\\beta}}}\\right]\n&=\\frac{1}{n}\\widehat{{\\mathbb{V}}}_{{\\boldsymbol{\\beta}}}^{\\text{HC0}} \\\\\n&=\\frac{1}{n}\\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{XX}}}^{-1}\\widehat{{\\mathbb{A}}}\\widehat{{\\mathbb{Q}}}_{{\\boldsymbol{XX}}}^{-1} \\\\\n&=\\frac{1}{n}\\left(\\frac{1}{n}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^T{\\boldsymbol{X}}_i\\right)^{-1}\n\\left(\\frac{1}{n}\\sum\\limits_{i=1}^ne_i^2{\\boldsymbol{X}}_i^T{\\boldsymbol{X}}_i\\right)\n\\left(\\frac{1}{n}\\sum\\limits_{i=1}^n{\\boldsymbol{X}}_i^T{\\boldsymbol{X}}_i\\right)^{-1} \\\\\n&=\\left({\\mathbb{X}}^T{\\mathbb{X}}\\right)^{-1}\n{\\mathbb{X}}^T{\\mathbb{D}}{\\mathbb{X}}\n\\left({\\mathbb{X}}^T{\\mathbb{X}}\\right)^{-1}\n\\end{aligned}\n where {\\mathbb{D}} is an n\\times n diagonal matrix with diagonal entries e_1^2,e_2^2,\\ldots,e_n^2. The estimator, \\widehat{{\\mathbb{V}}}\\left[\\widehat{{\\boldsymbol{\\beta}}}\\right], is referred to as the robust error variance estimator for the OLS coefficients \\widehat{{\\boldsymbol{\\beta}}}."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Jul 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/index.html#topological-data-analysis",
    "href": "tutorials/index.html#topological-data-analysis",
    "title": "Tutorials",
    "section": "",
    "text": "Jul 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/index.html#statistics-and-data-analysis",
    "href": "tutorials/index.html#statistics-and-data-analysis",
    "title": "Tutorials",
    "section": "Statistics and Data Analysis",
    "text": "Statistics and Data Analysis\n\n\n\n\n\n\n\n\nConfidence Intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\nLarge-Sample Regression Theory\n\n\n\nSep 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrder Statistics\n\n\n\nSep 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantile-Quantile Plots\n\n\n\nSep 15, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/index.html#miscellaneous",
    "href": "tutorials/index.html#miscellaneous",
    "title": "Tutorials",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\n\n\n\n\n\n\n\n\n\nLorenz System\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/miscellaneous/lorenz.html",
    "href": "tutorials/miscellaneous/lorenz.html",
    "title": "Lorenz System",
    "section": "",
    "text": "Lorenz system is a system of (non-linear) ordinary differential equations.\n\\begin{aligned}\n\\dot{x} &= \\sigma(y-x)\\\\\n\\dot{y} &= x(\\rho-z)-y\\\\\n\\dot{z} &= xy-\\beta z\n\\end{aligned}\nHere \\sigma,\\rho,\\text{ and }\\beta are system parameters."
  },
  {
    "objectID": "tutorials/miscellaneous/lorenz.html#the-system",
    "href": "tutorials/miscellaneous/lorenz.html#the-system",
    "title": "Lorenz System",
    "section": "",
    "text": "Lorenz system is a system of (non-linear) ordinary differential equations.\n\\begin{aligned}\n\\dot{x} &= \\sigma(y-x)\\\\\n\\dot{y} &= x(\\rho-z)-y\\\\\n\\dot{z} &= xy-\\beta z\n\\end{aligned}\nHere \\sigma,\\rho,\\text{ and }\\beta are system parameters."
  },
  {
    "objectID": "tutorials/miscellaneous/lorenz.html#evolution-projected-on-y-z-plane",
    "href": "tutorials/miscellaneous/lorenz.html#evolution-projected-on-y-z-plane",
    "title": "Lorenz System",
    "section": "Evolution Projected on Y-Z Plane",
    "text": "Evolution Projected on Y-Z Plane\n\nPlot.plot({\n  height: 500,\n  widht: 500,\n  x: {\n    domain: [-30, 30],\n    grid: true\n  },\n  y: {\n    domain: [0, 50],\n    grid: true\n  },\n  marks: [\n    Plot.line(lorenzData.slice(0, n), {\n      x: \"y\",\n      y: \"z\",\n      strokeWidth: 0.5,\n      stroke: \"red\"\n    }),\n    Plot.ruleX([0]),\n    Plot.ruleY([0])\n  ]\n})"
  },
  {
    "objectID": "tutorials/miscellaneous/lorenz.html#time-series-from-the-z-axis",
    "href": "tutorials/miscellaneous/lorenz.html#time-series-from-the-z-axis",
    "title": "Lorenz System",
    "section": "Time-series from the Z-axis",
    "text": "Time-series from the Z-axis\n\nPlot.plot({\n  x: {\n    domain: [0, 4000]\n  },\n  marks: [\n    Plot.line(lorenzData.slice(0, n), {\n      x: \"n\",\n      y: \"z\"\n    }),\n    Plot.ruleX([1500, 3500], { stroke: \"red\" }),\n    Plot.ruleY([0])\n  ]\n})"
  },
  {
    "objectID": "tutorials/miscellaneous/lorenz.html#takens-embedding",
    "href": "tutorials/miscellaneous/lorenz.html#takens-embedding",
    "title": "Lorenz System",
    "section": "Takens’ Embedding",
    "text": "Takens’ Embedding\n\nviewof delay = Inputs.number({ value: 7, label: tex`Delay\\ (\\tau)` })\nPlot.plot({\n  marks: [\n    Plot.dot(\n      delayTS(\n        lorenzData.map((d) =&gt; d.x),\n        delay\n      ).slice(0, n),\n      {\n        stroke: \"red\",\n        r: 1\n      }\n    )\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndelayTS = function (ts, delay = 1, dim = 2) {\n  return ts.map((d, i) =&gt; {\n    const out = [];\n    for (let j = 0; j &lt; dim; j++) out.push(ts[i + j * delay]);\n    return out;\n  });\n}\n\nlorenz = function (\n  [x, y, z] = [0, 0, 0],\n  n = 100,\n  { sigma = 10, r = 28, b = 8 / 3 } = {}\n) {\n  let F = function (t, [x, y, z]) {\n    return [sigma * (y - x), r * x - x * z - y, x * y - b * z];\n  };\n\n  let s = new odex.Solver(3);\n  let flow = [];\n\n  for (let i = 0; i &lt; n; i++) {\n    [x, y, z] = s.solve(F, 0, [x, y, z], 0.01).y;\n    flow.push({ n: i, x: x, y: y, z: z });\n  }\n  return flow;\n}\n\nlorenzData = lorenz([1, 1, 1], 4000)\n\nodex = import(\"https://cdn.skypack.dev/odex\");\n\nn = {\n  let i = 0;\n  while (i &lt; 4000) {\n    yield Promises.delay(10, ++i);\n  }\n}"
  },
  {
    "objectID": "tutorials/topology/reduction-action.html",
    "href": "tutorials/topology/reduction-action.html",
    "title": "Reduction in Action",
    "section": "",
    "text": "Code\n{\n  const container = d3\n    .create(\"div\")\n    .attr(\"class\", \"mat-container\")\n    .style(\"margin\", \"auto\");\n\n  container\n    .selectAll(\".row\")\n    .data(A)\n    .join(\"div\")\n    .attr(\"class\", \"row\")\n    .selectAll(\"div\")\n    .data((d) =&gt; d)\n    .join(\"div\")\n    .attr(\"class\", \"element\")\n    .style(\"background\", \"#eee\")\n    .style(\"width\", \"60px\")\n    .style(\"min-height\", \"60px\")\n    .style(\"padding\", \"5px\")\n    .style(\"margin\", \"2px\")\n    .style(\"text-align\", \"center\")\n    .style(\"border-radius\", \"5px\")\n    .style(\"display\", \"inline-block\")\n    .style(\"border\", \"2px solid white\")\n    .style(\"font-size\", \"15px\")\n    .style(\"line-height\", \"60px\")\n    .style(\"overflow\", \"hidden\")\n    .text((d) =&gt; d);\n  yield container.node();\n\n  let pivot,\n    antiPivot = [];\n  let offset = 0;\n  for (let i = 0; i &lt; steps.length; i++) {\n    let step = steps[i];\n\n    if (step.name === \"offset\") {\n      container\n        .selectAll(\".row\")\n        .filter((d, k) =&gt; k === step.offset)\n        .selectAll(\".element\")\n        .style(\"background\", \"#BDF3C2\");\n      container\n        .selectAll(\".row\")\n        .selectAll(\".element\")\n        .filter((d, k) =&gt; k === step.offset)\n        .style(\"background\", \"#BDF3C2\");\n\n      offset = step.offset + 1;\n    }\n\n    clean(offset);\n\n    if (step.pivot) {\n      pivot = [...step.pivot];\n      showPivot(pivot);\n    }\n    if (step.antiPivot) {\n      antiPivot = [...step.antiPivot];\n      showAntiPivot(antiPivot);\n    }\n\n    await Promises.delay(delay);\n\n    if (step.name === \"exchangeRows\") {\n      container\n        .selectAll(\".row\")\n        .filter((d, k) =&gt; k === step.args[0] || k === step.args[1])\n        .selectAll(\".element\")\n        .filter((d, k) =&gt; k &gt;= offset)\n        .style(\"background\", \"beige\");\n      await Promises.delay(delay);\n\n      pivot[0] = step.offset;\n      showPivot(pivot);\n    } else if (step.name === \"exchangeCols\") {\n      container\n        .selectAll(\".row\")\n        .filter((d, k) =&gt; k &gt;= offset)\n        .selectAll(\".element\")\n        .filter((d, k) =&gt; k === step.args[0] || k === step.args[1])\n        .style(\"background\", \"beige\");\n      await Promises.delay(delay);\n      pivot[1] = step.offset;\n      showPivot(pivot);\n    } else if (step.name === \"replaceRow\") {\n      container\n        .selectAll(\".row\")\n        .filter((d, k) =&gt; k === step.args[0] || k === step.args[1])\n        .selectAll(\".element\")\n        .filter((d, k) =&gt; k &gt;= offset)\n        .style(\"background\", \"beige\");\n      await Promises.delay(delay);\n    } else if (step.name === \"replaceCol\") {\n      container\n        .selectAll(\".row\")\n        .filter((d, k) =&gt; k &gt;= offset)\n        .selectAll(\".element\")\n        .filter((d, k) =&gt; k === step.args[0] || k === step.args[1])\n        .style(\"background\", \"beige\");\n      await Promises.delay(delay);\n    }\n\n    container\n      .selectAll(\".row\")\n      .data(step.mat)\n      .selectAll(\"div\")\n      .data((d) =&gt; d)\n      .html((d) =&gt; d);\n  }\n  container\n    .selectAll(\".row\")\n    .selectAll(\".element\")\n    .style(\"border\", \"2px solid white\");\n\n  function clean(offset) {\n    container\n      .selectAll(\".row\")\n      .filter((d, k) =&gt; k &gt;= offset)\n      .selectAll(\".element\")\n      .filter((d, k) =&gt; k &gt;= offset)\n      .style(\"background\", \"#eee\");\n  }\n\n  function showPivot([i, j]) {\n    container\n      .selectAll(\".row\")\n      .selectAll(\".element\")\n      .style(\"border\", \"2px solid white\");\n    container\n      .selectAll(\".row\")\n      .filter((d, k) =&gt; k === i)\n      .selectAll(\".element\")\n      .filter((d, k) =&gt; k === j)\n      .style(\"border\", \"3px solid green\");\n  }\n  function showAntiPivot([i, j]) {\n    container\n      .selectAll(\".row\")\n      .filter((d, k) =&gt; k === i)\n      .selectAll(\".element\")\n      .filter((d, k) =&gt; k === j)\n      .style(\"border\", \"3px solid red\");\n  }\n}\n\n\n\n\n\n\n\nFor more details, see: Normal Form.\n\n\nCode\ndelay = 300\n\nA = {\n  let rows = 7; //d3.randomInt(1, 10)();\n  let cols = 7; //d3.randomInt(1, 10)();\n\n  return d3.range(0, rows).map((row) =&gt; {\n    return d3.range(0, cols).map((elm) =&gt; d3.randomInt(-3, 3)());\n  });\n}\n\nnf = require(\"https://bundle.run/@tdajs/normal-form@2.0.0\")\n\nsteps = new nf.NormalForm(A, {\n  recordSteps: true\n}).steps"
  },
  {
    "objectID": "tutorials/topology/abelian-groups/index.html",
    "href": "tutorials/topology/abelian-groups/index.html",
    "title": "Finitely-Generated Free Abelian Groups",
    "section": "",
    "text": "Let (G,+) and (G',+) be finitely-generated free (additive) abelian groups of dimension m and n, respectively. We choose arbitrary ordered bases \\mathcal{E}=(e_1,e_2,\\ldots,e_m) and \\mathcal{E}'=(e_1',e_2',\\ldots,e_n') for G and G', respectively."
  },
  {
    "objectID": "tutorials/topology/abelian-groups/index.html#matrix-of-a-homomorphism",
    "href": "tutorials/topology/abelian-groups/index.html#matrix-of-a-homomorphism",
    "title": "Finitely-Generated Free Abelian Groups",
    "section": "Matrix of A Homomorphism",
    "text": "Matrix of A Homomorphism\nA homomorphism F:G\\to G' is a linear function, i.e., F(x+y)=F(x)+F(y) for all x,y\\in G.\nAs a consequence, F is uniquely determined by its values on the elements of \\mathcal{E}. For each 1\\leq j\\leq m, the image F(e_j) can be written uniquely as a (formal) linear combination of the elements of \\mathcal{E}'. Let \nF(e_j) = \\sum\\limits_{i=1}^{n} a_{ij}e_i'.\n\nThe matrix A:=\\{a_{ij}\\}\\in\\mathcal{M}_{n,m}(\\mathbb Z) is called the matrix of F w.r.t. the (ordered) bases \\mathcal{E},\\mathcal{E}', and is denoted by [F]_{\\mathcal{E},\\mathcal{E'}}. In matrix notation, we have \n\\bigg[F(e_1),\\ldots,F(e_j),\\ldots,F(e_m)\\bigg]=\n\\bigg[e_1',\\ldots,e_i',\\ldots,e_n'\\bigg]\n\\begin{bmatrix}\na_{11} & \\ldots & a_{1j} & \\ldots & a_{1m} \\\\\n\\ldots & \\ldots & \\ldots & \\ldots & \\ldots \\\\\na_{i1} & \\ldots & a_{ij} & \\ldots & a_{im} \\\\\n\\ldots & \\ldots & \\ldots & \\ldots & \\ldots \\\\\na_{n1} & \\ldots & a_{nj} & \\ldots & a_{nm} \\\\\n\\end{bmatrix}.\n Tolerating a mild abuse of notation, we can also express the above relation as F(\\mathcal{E})=\\mathcal{E'}[F]_{\\mathcal{E},\\mathcal{E'}}.\n\nExample:\nLet F:\\mathbb{Z}^3\\to \\mathbb{Z}^2 be a homomorphism defined by F(a,b,c)=(3a+4b-2c,2b+5c).\nA natural choice of ordered bases is \\mathcal{E}=\\bigg((1,0,0),(0,1,0),(0,0,1)\\bigg) and \\mathcal{E}'=\\bigg((1,0),(0,1)\\bigg). In order to compute [F]_{\\mathcal{E},\\mathcal{E}''}, the matrix of F w.r.t. the chosen bases, we note that \n\\begin{aligned}\nF(1,0,0) &= (3,0) = 3(1,0) + 0(0,1) \\\\\nF(0,1,0) &= (4,2) = 4(1,0) + 2(0,1) \\\\\nF(0,0,1) &= (-2,5) = -2(1,0) + 5(0,1)\n\\end{aligned}\n So, the matrix of F is \n\\begin{bmatrix}\n3 & 4 & -2\\\\\n0 & 2 & 5\n\\end{bmatrix}.\n\nQuestion What would the matrix of F be if we chose the bases \\bigg((1,1,0),(1,0,1),(0,1,1)\\bigg) and \\bigg((1,0),(1,1)\\bigg)?\n\nFollowing the technique (basically the definition) of the above example we can compute the matrix of F under the new set of bases."
  },
  {
    "objectID": "tutorials/topology/abelian-groups/index.html#change-of-basis",
    "href": "tutorials/topology/abelian-groups/index.html#change-of-basis",
    "title": "Finitely-Generated Free Abelian Groups",
    "section": "Change of Basis",
    "text": "Change of Basis\nLet \\mathcal{B}=(e_1,e_2,\\ldots,e_m) be an ordered basis of a finitely-generated free abelian group G. Now we consider an new basis \\mathcal{C}=(f_1,f_2,\\ldots,f_m) of G. The basechange matrix encodes the change the basis from (old) \\mathcal{B} to (new) \\mathcal{C}. The basechange matrix facilitates the computation of the matrix of a homomomorphism w.r.t. a set of new bases.\nThe starting point is to write the elements of the new basis as a linear combination of those of the old basis. For j=1,2,\\ldots,m', \nf_j = \\sum\\limits_{i=1}^{m} p_{ij}e_i.\n\nThe matrix P is called the basechange matrix the order bases \\mathcal{B}\\text{ and }\\mathcal{C}. In matrix notation, \\mathcal{C}=\\mathcal{B}P.\nAs we can immediately see, the basechange matrix is unique, and it is also invertible [artin], i.e., P\\in GL_{m}(\\mathbb Z). Given an invertible, integer matrix, one can also use the above matrix relation to compute the new basis.\n\nExample:\nFor this example, I let you interact with the tutorial. We choose the dimension of G using the following slider. Upon choosing a value, you readily see the initial basis:\n\ntex`\\mathcal{B}=${nf.vec2Tex(B)}`\n\n\n\n\n\n\nand randomly generated basechange matrix P.\n\nviewof m = Inputs.range([1, 6], {\n  value: 3,\n  step: 1,\n  label: \"Dimension (m):\"\n})\n\n\n\n\n\n\nNow, we take a random invertible matrix of the chosen dimension:\n\ntex.block`P=${nf.mat2Tex(P)}.`\n\n\n\n\n\n\nThe new basis of G is:\n\ntex.block`\\mathcal{C}=${nf.vec2Tex(nf.changeBasis(B,P))}.`\n\n\n\n\n\n\n\n\nApplication to Homomorphisms\nLet F:G\\to G' be a homomorphism. Let A denote the matrix of F w.r.t. a pair of old ordered bases (\\mathcal{B},\\mathcal{B}') for G and G', respectively. If a new pair of order bases (\\mathcal{C},\\mathcal{C}') chosen, one would be interested to compute the matrix of F w.r.t. the new bases.\nLet P\\in\\mathcal{M}_m(\\mathbb Z) and Q\\in\\mathcal{M}_n(\\mathbb Z) denote the basechange matrices for G and G', respectively. So, \\mathcal{C}=\\mathcal{B}P\\text{, and }\\mathcal{C}'=\\mathcal{B}'Q.\nBy the definition of A, note that F(\\mathcal{B})=\\mathcal{B}'A. Therefore, \n\\begin{aligned}\nF(\\mathcal{B})P &=(\\mathcal{B}'A)P \\\\\n&= (\\mathcal{C}'Q^{-1})AP \\\\\n&= \\mathcal{C}'(Q^{-1}AP)\n\\end{aligned}\n\nUsing the fact that F is a homomomorphism and letting \\mathcal{B}=(e_1,e_2,\\ldots,e_m) and \\mathcal{C}=(f_1,f_2,\\ldots,f_m), we simplify the LHS. \n\\begin{aligned}\nF(\\mathcal{B})P &= \\big[F(e_1),\\ldots,F(e_j),\\ldots,F(e_m)\\big]P \\\\\n&= \\left[\\sum_{j=1}^mF(e_j)p_{1j},\\ldots,\\sum_{j=1}^mF(e_j)p_{mj}\\right] \\\\\n&= \\left[F\\left(\\sum_{j=1}^m p_{1j}e_j\\right),\\ldots,F\\left(\\sum_{j=1}^m p_{1j}e_j\\right)\\right] \\\\\n&= \\left[F\\left(f_1\\right),\\ldots,F\\left(f_m\\right)\\right] \\\\\n&= F(\\mathcal{C}).\n\\end{aligned}\n\nTherefore, F(\\mathcal{C})=\\mathcal{C}'\\left(Q^{-1}AP\\right).\nWe conclude that \\left(Q^{-1}AP\\right) is the matrix of F w.r.t. the new pair of basis (\\mathcal{C},\\mathcal{C}').\n\nnf = require(\"https://bundle.run/@tdajs/normal-form@2.1.0\")\nB = d3.range(0, m).map((e) =&gt; \"e_\" + (e + 1))\nP = {\n  let P = [];\n  do {\n    P = d3.range(0, m).map((row) =&gt; {\n      return d3.range(0, m).map((elm) =&gt; d3.randomInt(-5, 5)());\n    });\n  } while (new nf.NormalForm(P).D.includes(0));\n\n  return P;\n}"
  },
  {
    "objectID": "research/publications/index.html",
    "href": "research/publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Preprints\n\n\n\n\n\n\n\n\nApplied Topology\n\n\nComputational Topology\n\n\nTDA\n\n\nManifold Learning\n\n\nTopological Stability and Latschev-type Reconstruction Theorems for \\pmb{\\mathrm{CAT}(\\kappa)} Spaces\n\nSubmitted to: Advances in Mathematics, 2024 With: Rafal KomendarczykLinks: [arxiv] \n\n\n\n\n\n\n\n\nApplied Topology\n\n\nComputational Topology\n\n\nHausdorff vs Gromov–Hausdorff Distances\n\nSubmitted to: Discrete & Computational Geometry, 2023 With: Henry Adams, Florian Frick, and Nicholas McBrideLinks: [arxiv] \n\n\n\n\nNo matching items\n\n\n\n\nJournals\n\n\n\n\n\n\n\n\nStatistical Finance\n\n\nComplex Network Analysis of Cryptocurrency Market During Crashes\n\nPhysica A, 2024 With: Kundan Mukhia, Anish Rai, SR Luwang, Md Nurujjaman, and Chittaranjan HensLinks: [arxiv] [publisher] \n\n\n\n\n\n\n\n\nApplied Topology\n\n\nManifold Learning\n\n\nDemystifying Latschev's Theorem: Manifold Reconstruction from Noisy Data\n\nDiscrete & Computational Geometry, 2024 Links: [arvix] [publisher] \n\n\n\n\n\n\n\n\nComputational Geometry\n\n\nPattern Matching\n\n\nDistance Measures for Geometric Graphs\n\nComputational Geometry: Theory and Applications, 2023 With: Carola WenkLinks: [arxiv] [publisher] \n\n\n\n\n\n\n\n\nApplied Topology\n\n\nComputational Topology\n\n\nTDA\n\n\nVietoris–Rips Complexes of Metric Spaces Near a Metric Graph\n\nJournal of Applied and Computational Topology, 2023 Links: [arxiv] [publisher] \n\n\n\n\n\n\n\n\nComputational Geometry\n\n\nPattern Recognition\n\n\nApproximating Gromov-Hausdorff Distance in Euclidean Space\n\nComputational Geometry: Theory and Applications, 2022 With: Jeffrey Vitter and Carola WenkLinks: [publisher] [arxiv] \n\n\n\n\n\n\n\n\nComputational Geometry\n\n\nApplied Topology\n\n\nTDA\n\n\nOn the Reconstruction of Geodesic Subspaces of \\pmb{\\mathbb R^n}\n\nInternational Journal of Computational Geometry and Applications, 2022 With: Brittany Fasy, Rafal Komendaczyk, and Carola WenkLinks: [publisher] [arxiv] \n\n\n\n\n\n\n\n\nStatistical Finance\n\n\nA Sentiment-Based Modeling and Analysis of Stock Price During the COVID-19: U- and Swoosh-Shaped Recovery\n\nPhysica A: Statistical Mechanics and Its Applications, 2021 With: Anish Rai, AjitMahata, Md Nurujjaman, and Kanish DebnathLinks: [publisher] [arxiv] \n\n\n\n\nNo matching items\n\n\n\n\nProceedings of Conferences and Workshops\n\n\n\n\n\n\n\n\nApplied Topology\n\n\nManifold Learning\n\n\nDemystifying Latschev's Theorem: Manifold Reconstruction from Noisy Data\n\nSymposium on Computational Geometry (SoCG), 2024 Links: [publisher] \n\n\n\n\n\n\n\n\nComputational Geometry\n\n\nMetric and Path-Connectedness Properties of the Fréchet Distance for Paths and Graphs\n\nCanadian Conference on Computational Geometry (CCCG), 2023 With: Erin Chambers, Brittany Fasy, Benjamin Holmgren, and Carola WenkLinks: [FILE] \n\n\n\n\n\n\n\n\nComputational Geometry\n\n\nPattern Recognition\n\n\nGraph Mover's Distance: An Efficiently Computable Distance Measure for Geometric Graphs\n\nCanadian Conference on Computational Geometry (CCCG), 2023 Links: [ResearchGate] [arxiv] \n\n\n\n\n\n\n\n\nTDA\n\n\nThreshold-based graph reconstruction using discrete Morse theory\n\nFall Workshop on Computational Geometry, New York, NY, November 2018, 2018 With: Brittany Terese Fasy and Carola WenkLinks: [arxiv] \n\n\n\n\n\n\n\n\nTDA\n\n\nTopological and Geometric Reconstruction of Metric Graphs in \\mathbb{R}^n\n\nFall Workshop on Computational Geometry*, New York, NY, October 2017, 2017 With: Brittany Terese Fasy, Rafal Komendaczyk, and Carola WenkLinks: [proceedings] [arxiv] \n\n\n\n\nNo matching items\n\n\n\n\nPhD Thesis\n\n\n    \n    \n    \n    TDA\n            Applied Topology\n            Pattern Matching\n            Computational Geometry\n    Topological Methods in Shape Reconstruction and Comparison  \n    Mathematics Department, Tulane University, December 2020\n    Link: Thesis\n    \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "research/projects/shape-recon.html",
    "href": "research/projects/shape-recon.html",
    "title": "Shape Reconstruction",
    "section": "",
    "text": "Rafal Komendarczyk\nAtish Mishra"
  },
  {
    "objectID": "research/projects/shape-recon.html#collaborators",
    "href": "research/projects/shape-recon.html#collaborators",
    "title": "Shape Reconstruction",
    "section": "",
    "text": "Rafal Komendarczyk\nAtish Mishra"
  },
  {
    "objectID": "research/projects/shape-recon.html#publications",
    "href": "research/projects/shape-recon.html#publications",
    "title": "Shape Reconstruction",
    "section": "Publications",
    "text": "Publications\n\n\n[1] B. T. Fasy, R. Komendarczyk, S. Majhi, and C. Wenk, “On the reconstruction of geodesic subspaces of \\mathbb{R}^N,” International Journal of Computational Geometry & Applications, vol. 32, no. 01n02, pp. 91–117, 2022, doi: 10.1142/S0218195922500066. Available: https://doi.org/10.1142/S0218195922500066\n\n\n[2] S. Majhi, “Vietoris–Rips complexes of metric spaces near a metric graph,” J. Appl. Comput. Topol., vol. 7, no. 4, pp. 741–770, Dec. 2023.\n\n\n[3] S. Majhi, “Demystifying latschev’s theorem: Manifold reconstruction from noisy data,” Discrete Comput. Geom., May 2024.\n\n\n[4] S. Majhi, “Demystifying Latschev’s Theorem: Manifold Reconstruction from Noisy Data,” in 40th international symposium on computational geometry (SoCG 2024), W. Mulzer and J. M. Phillips, Eds., in Leibniz international proceedings in informatics (LIPIcs), vol. 293. Dagstuhl, Germany: Schloss Dagstuhl – Leibniz-Zentrum für Informatik, 2024, pp. 73:1–73:16. doi: 10.4230/LIPIcs.SoCG.2024.73. Available: https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.SoCG.2024.73\n\n\n[5] R. Komendarczyk, S. Majhi, and W. Tran, “Topological stability and latschev-type reconstruction theorems for \\boldsymbol{\\mathrm{CAT}(\\kappa)} spaces,” 2024."
  },
  {
    "objectID": "research/projects/stock-market.html",
    "href": "research/projects/stock-market.html",
    "title": "Stock Market Analysis",
    "section": "",
    "text": "This is a joint project with Md. Nurujjaman’s group at the National Institute of Technology, India. In the aftermath of stock market crash due to COVID-19, not all sectors recovered in the same way. In , we proposed novel models to capture the different types of recovery profiles for Indian stocks. We also employed the Empirical Mode Decomposition (EMD) for a statistical significance analysis of our model.\nSo far, the purpose of our models has been descriptive, however prediction of market crash is more crucial and challenging than just observing it. We are now pushing the project in the direction of predicting a future crash in a financial sector—using tools from TDA. Although TDA is not a conventional tool for analyzing non-linear time series, we are emboldened by its recent success in such applications."
  },
  {
    "objectID": "research/projects/stock-market.html#publications",
    "href": "research/projects/stock-market.html#publications",
    "title": "Stock Market Analysis",
    "section": "Publications",
    "text": "Publications\n\n\n[1] K. Mukhia, A. Rai, S. R. Luwang, Nurujjaman, S. Majhi, and C. Hens, “Complex network analysis of cryptocurrency market during crashes,” 2024.\n\n\n[2] A. Rai, B. N. Sharma, S. R. Luwang, Nurujjaman, and S. Majhi, “Identifying extreme events in the stock market: A topological data analysis,” 2024.\n\n\n[3] A. Rai, A. Mahata, M. Nurujjaman, S. Majhi, and K. Debnath, “A sentiment-based modeling and analysis of stock price during the COVID-19: U- and swoosh-shaped recovery,” Physica A: Statistical Mechanics and its Applications, vol. 592, p. 126810, 2022, doi: https://doi.org/10.1016/j.physa.2021.126810. Available: https://www.sciencedirect.com/science/article/pii/S0378437121009778"
  },
  {
    "objectID": "research/projects/gh.html",
    "href": "research/projects/gh.html",
    "title": "Gromov–Hausdorff Distances",
    "section": "",
    "text": "Henry Adams\nNicolò Zava\nŽiga Virk\nFedya\nFlorian Frick (past)\nNicolas McBride (past)"
  },
  {
    "objectID": "research/projects/gh.html#collaborators",
    "href": "research/projects/gh.html#collaborators",
    "title": "Gromov–Hausdorff Distances",
    "section": "",
    "text": "Henry Adams\nNicolò Zava\nŽiga Virk\nFedya\nFlorian Frick (past)\nNicolas McBride (past)"
  },
  {
    "objectID": "research/projects/gh.html#publications",
    "href": "research/projects/gh.html#publications",
    "title": "Gromov–Hausdorff Distances",
    "section": "Publications",
    "text": "Publications\n\n\n[1] S. Majhi, J. Vitter, and C. Wenk, “Approximating Gromov–Hausdorff distance in Euclidean space,” Computational Geometry, vol. 116, p. 102034, 2024, doi: https://doi.org/10.1016/j.comgeo.2023.102034\n\n\n[2] H. Adams, F. Frick, S. Majhi, and N. McBride, “Hausdorff vs Gromov–Hausdorff distances,” arXiv preprint arXiv:2309.16648, 2023, Available: https://arxiv.org/abs/2309.16648"
  },
  {
    "objectID": "research/projects/ggd.html",
    "href": "research/projects/ggd.html",
    "title": "Distance Measures for Geometric Graphs",
    "section": "",
    "text": "The project broadly looks into different topology- and geometry-inspired graphical signatures for easy description and comparison of complex data. One such powerful signature is geometric graphs. A geometric graph is a combinatorial graph, endowed with a geometry that is inherited from its embedding in a Euclidean space.\nFormulation of a meaningful measure of (dis-)similarity in both the combinatorial and geometric structures of two such geometric graphs is a challenging problem in pattern recognition. In [1], I presented two notions of distance measures for geometric graphs: the geometric edit distance (GED) and geometric graph distance (GGD). While the former is based on the idea of editing one graph to transform it into the other graph, the latter is inspired by the inexact matching of the graphs. Alongside studying the metric properties of GED and GGD, I investigated how the two notions compare. I proved a very strong result showing that both the distances are \\mathcal{NP}-hard to compute, even if the graphs are planar and arbitrary cost coefficients are allowed.\nIn order to provide an alternative distance measure for graphs, I also worked with Ben Holmgren, who was an undergraduate student at Montana State University at the time. In [2], we presented various topological properties of the space of graphs under the Fréchet distance."
  },
  {
    "objectID": "research/projects/ggd.html#collaborators",
    "href": "research/projects/ggd.html#collaborators",
    "title": "Distance Measures for Geometric Graphs",
    "section": "Collaborators",
    "text": "Collaborators\n\nVitaliy Kurlin\nCarola Wenk (past)\nErin Chambers (past)\n\n\nGraduate Mentees\n\nKhush Shah (GWU)\nShikha Kumari (GWU)"
  },
  {
    "objectID": "research/projects/ggd.html#publications",
    "href": "research/projects/ggd.html#publications",
    "title": "Distance Measures for Geometric Graphs",
    "section": "Publications",
    "text": "Publications\n\n\n[1] S. Majhi and C. Wenk, “Distance measures for geometric graphs,” Computational Geometry, vol. 118, p. 102056, 2024, doi: https://doi.org/10.1016/j.comgeo.2023.102056\n\n\n[2] E. Chambers, B. Fasy, B. Holmgren, S. Majhi, and C. Wenk, “Metric and path-connectedness properties of the fréchet distance for paths and graphs,” in Proceedings of the 34th Canadian Conference on Computational Geometry, CCCG 2022, Concordia University, Montreal, QC, Canada, july 31-august 2, 2023, pp. 213–220. Available: https://wadscccg2023.encs.concordia.ca/assets/pdf/CCCG_2023_proc.pdf\n\n\n[3] S. Majhi, “Graph mover’s distance: An efficiently computable distance measure for geometric graphs,” in Proceedings of the 34th Canadian Conference on Computational Geometry, CCCG 2022, Concordia University, Montreal, QC, Canada, july 31-august 2, 2023, pp. 207–212. Available: https://wadscccg2023.encs.concordia.ca/assets/pdf/CCCG_2023_proc.pdf"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "My research revolves around the interface of mathematics and computer science, with a focus on the mathematical foundation of data science. More specifically, I am interested in applied topology, computational geometry, and topological data analysis (TDA). The theoretical side of my research concerns developing inference techniques for data science that are inspired by topological and geometric methods. As a topological data analyst, I am also motivated to solve practical problems arising in various fields, like finance, physics, biology, and medicine."
  },
  {
    "objectID": "research/index.html#research-projects",
    "href": "research/index.html#research-projects",
    "title": "Research",
    "section": "Research Projects",
    "text": "Research Projects\n\n\n\n\n\n\n\n\n\n\nDistance Measures for Geometric Graphs\n\n\n\n\n\n\n\n\n\n\n\n\n\nGromov–Hausdorff Distances\n\n\n\n\n\n\n\n\n\n\n\n\n\nShape Reconstruction\n\n\n\n\n\n\n\n\n\n\n\n\n\nStock Market Analysis\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#publications",
    "href": "research/index.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\nTo see all my publications, visit the publications page."
  },
  {
    "objectID": "tutorials/topology/smith/index.html",
    "href": "tutorials/topology/smith/index.html",
    "title": "Smith Normal Form",
    "section": "",
    "text": "Matrix reduction plays a fundamental role in the study of vector spaces and linear transformations. Reduction of a matrix to a canonical or normal form finds its use in almost all applied fields of science. While there are many different normal forms a matrix can be reduced to, a normal form is usually the product of much simpler and well-understood (e.g., diagonal, upper/lower triangular, etc) matrices. Since, the reduction process faithfully carries most of the nice properties of the original matrix, one can use a normal form to infer some of those properties from matrices with relatively simple structure.\nThe choice of the normal form can sometimes be demanded by a particular application, or be limited by the algebraic operations one is allowed to perform on the entries of the original matrix. Today we consider, for reduction, only \\mathbb Z-matrices, i.e., matrices that are allowed to have only integer entries. As a consequence, the only permitted operations on the entries of such a matrix are addition and multiplication—but not division. We denote by \\mathcal{M}_{n,m}(\\mathbb Z) the set of all integer matrices with n rows and m columns. In the special case of square matrices (m=n), we simply call it \\mathcal{M}_{m}(\\mathbb Z) or \\mathcal{M}_{n}(\\mathbb Z). Note that the identity matrix, I_{m}, of dimension m is a \\mathbb Z-matrix, i.e., I_m\\in\\mathcal{M}_{m}(\\mathbb Z).\n\n\nAlthough our hands are now tied by the restriction on the operations allowed on a matrix A\\in\\mathcal{M}_{n,m}(\\mathbb Z), we can still follow the usual definitions of matrix addition, subtraction, and multiplication— also determinant and inverse of a square matrix. For a square matrix A\\in\\mathcal{M}_{n}(\\mathbb Z), its determinant is defined the usual way. Also, A is said to have an inverse B\\in\\mathcal{M}_{n}(\\mathbb Z) if AB=BA=I_n. The set of all invertible (square) matrices of dimension n is denoted by GL_{n}(\\mathbb Z).\n\n\n\nGiven an A\\in\\mathcal{M}_{n,m}(\\mathbb Z), we seek invertible matrices P\\in GL_{m}(\\mathbb Z) and Q\\in GL_{n}(\\mathbb Z) such that D=Q^{-1}AP, where D is an n\\times m diagonal integer matrix: \n\\newcommand\\bigzero{\\makebox(0,0){\\text{\\huge0}}}\n\\begin{pmatrix}\n\\begin{array}{c:c}\n  \\begin{matrix}\n    d_1 & 0 & \\ldots & 0 \\\\\n    0 & d_2 & \\ldots & 0\\\\\n    0 & \\ldots & \\ldots & 0 \\\\\n    0 & \\ldots & 0 & d_k \\\\\n  \\end{matrix}\n  & {\\huge0}_{k,m-k} \\\\\n  \\hdashline \\\\\n  {\\huge0}_{n-k,k} & {\\huge0}_{n-k,m-k}\n\\end{array}\n\\end{pmatrix}\n with d_i\\geq0 for all i=1, 2, \\ldots, k and d_1| d_2| \\ldots | d_k.\nNote: - the diagonal matrix D always exists, as we allow it to be a zero matrix; it is also unique. - P,Q may not, however, be unique [artin]. - if A is viewed as the matrix of a linear transformation T:G_1\\to G_2 between two finitely-generated free abelian groups, then P and Q are the basechange matrices for G_1 and G_2, respectively.\n\n\n\nUnlike the other popular canonical forms—e.g., the Jordan normal form for matrices over an algebraically-closed field—the Smith normal form is particularly relevant in the context of \\mathbb Z-modules, more generally R-modules [artin]. In the field of combinatorial topology, the reduction to Smith normal form facilitates the computation of the homology groups of finte simplicial complexes [Munkres84]."
  },
  {
    "objectID": "tutorials/topology/smith/index.html#introduction",
    "href": "tutorials/topology/smith/index.html#introduction",
    "title": "Smith Normal Form",
    "section": "",
    "text": "Matrix reduction plays a fundamental role in the study of vector spaces and linear transformations. Reduction of a matrix to a canonical or normal form finds its use in almost all applied fields of science. While there are many different normal forms a matrix can be reduced to, a normal form is usually the product of much simpler and well-understood (e.g., diagonal, upper/lower triangular, etc) matrices. Since, the reduction process faithfully carries most of the nice properties of the original matrix, one can use a normal form to infer some of those properties from matrices with relatively simple structure.\nThe choice of the normal form can sometimes be demanded by a particular application, or be limited by the algebraic operations one is allowed to perform on the entries of the original matrix. Today we consider, for reduction, only \\mathbb Z-matrices, i.e., matrices that are allowed to have only integer entries. As a consequence, the only permitted operations on the entries of such a matrix are addition and multiplication—but not division. We denote by \\mathcal{M}_{n,m}(\\mathbb Z) the set of all integer matrices with n rows and m columns. In the special case of square matrices (m=n), we simply call it \\mathcal{M}_{m}(\\mathbb Z) or \\mathcal{M}_{n}(\\mathbb Z). Note that the identity matrix, I_{m}, of dimension m is a \\mathbb Z-matrix, i.e., I_m\\in\\mathcal{M}_{m}(\\mathbb Z).\n\n\nAlthough our hands are now tied by the restriction on the operations allowed on a matrix A\\in\\mathcal{M}_{n,m}(\\mathbb Z), we can still follow the usual definitions of matrix addition, subtraction, and multiplication— also determinant and inverse of a square matrix. For a square matrix A\\in\\mathcal{M}_{n}(\\mathbb Z), its determinant is defined the usual way. Also, A is said to have an inverse B\\in\\mathcal{M}_{n}(\\mathbb Z) if AB=BA=I_n. The set of all invertible (square) matrices of dimension n is denoted by GL_{n}(\\mathbb Z).\n\n\n\nGiven an A\\in\\mathcal{M}_{n,m}(\\mathbb Z), we seek invertible matrices P\\in GL_{m}(\\mathbb Z) and Q\\in GL_{n}(\\mathbb Z) such that D=Q^{-1}AP, where D is an n\\times m diagonal integer matrix: \n\\newcommand\\bigzero{\\makebox(0,0){\\text{\\huge0}}}\n\\begin{pmatrix}\n\\begin{array}{c:c}\n  \\begin{matrix}\n    d_1 & 0 & \\ldots & 0 \\\\\n    0 & d_2 & \\ldots & 0\\\\\n    0 & \\ldots & \\ldots & 0 \\\\\n    0 & \\ldots & 0 & d_k \\\\\n  \\end{matrix}\n  & {\\huge0}_{k,m-k} \\\\\n  \\hdashline \\\\\n  {\\huge0}_{n-k,k} & {\\huge0}_{n-k,m-k}\n\\end{array}\n\\end{pmatrix}\n with d_i\\geq0 for all i=1, 2, \\ldots, k and d_1| d_2| \\ldots | d_k.\nNote: - the diagonal matrix D always exists, as we allow it to be a zero matrix; it is also unique. - P,Q may not, however, be unique [artin]. - if A is viewed as the matrix of a linear transformation T:G_1\\to G_2 between two finitely-generated free abelian groups, then P and Q are the basechange matrices for G_1 and G_2, respectively.\n\n\n\nUnlike the other popular canonical forms—e.g., the Jordan normal form for matrices over an algebraically-closed field—the Smith normal form is particularly relevant in the context of \\mathbb Z-modules, more generally R-modules [artin]. In the field of combinatorial topology, the reduction to Smith normal form facilitates the computation of the homology groups of finte simplicial complexes [Munkres84]."
  },
  {
    "objectID": "tutorials/topology/smith/index.html#elementary-operations",
    "href": "tutorials/topology/smith/index.html#elementary-operations",
    "title": "Smith Normal Form",
    "section": "Elementary Operations",
    "text": "Elementary Operations\nAs stated above without a proof that the reduction of an integer matrix to a Smith normal form always exists. We now present the steps one can perform to decompose a given matrix. The basis of the reduction process involves three elementary operations. We invoke them repeatedly on the original matrix in the right order. We are going to use three elementary row operations and three corresonding column operations.\n\nElementary Matrices\nEach of the following row (and column) operations can also be described as pre (and post) multiplication by an elementary matrix. For each elementary operation, there is an associated elementary matrix, which is obtained by performing the same operation on the identity matrix of the right size.\n\n\nRow Operations\nWe first define and then demonstrate the elementary row operations. The outcome A' of each row operation on A is a pre-multiplication of A by an elementary matrix E, i.e., A'=EA. The three types of row operations are as follows:\n\nMultiply the i-th row by -1\nExchange the i-th and j-th row\nReplace the i-th row with the row plus q times the j-th row (i\\neq j, q\\neq 0)\n\n\n\nColumn Operations\nThe outcome A' of each column operation on A is post-multiplication of A by an elementary matrix E, i.e., A'=AE. The three types of column operations are as follows:\n\nMultiply the i-th column by -1\nExchange the i-th and j-th column\nReplace i-th column with the column plus q times the j-th column (i\\neq j, q\\neq 0)\n\n\n\nDemo\nThe operations are best understood when demonstrated on an example matrix. To that end, we first generate a random matrix A with integer (between -5 and 5) entries by choosing the number of rows and columns using the sliders.\n\n\nCode\nviewof m = Inputs.range([1, 6], {\n  value: 5,\n  step: 1,\n  label: \"Number of rows (n):\"\n})\nviewof n = Inputs.range([1, 6], {\n  value: 4,\n  step: 1,\n  label: \"Number of cols (m):\"\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you do not like the random matrix, try fiddling with the above sliders to generate a new one!\n\n\nCode\ntex.block`A=${nj.mat2Tex(A)}`\n\n\n\n\n\n\n\nSee for yourself: From the dropdown below choose the operation type (row/column) and an operation. Also, set the values of the arguemts for the operations chosen, using the sliders.\n\n\nCode\nviewof opType = Inputs.select([\"row\", \"col\"], {\n  label: \"Type of operation\"\n})\nviewof operation = Inputs.select(\n  Operations[opType].map((o) =&gt; o.name),\n  {\n    label: \"operation\"\n  }\n)\nviewof i = Inputs.range([1, opType === \"row\" ? n : m], {\n  value: 1,\n  step: 1,\n  label: tex`i`\n})\nviewof j = Inputs.range([1, opType === \"row\" ? n : m], {\n  value: 2,\n  step: 1,\n  label: tex`j`\n})\nviewof q = Inputs.range([-20, 20], {\n  value: 10,\n  step: 1,\n  label: tex`q`\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is the outcome matrix:\n\n\nCode\ntex.block`A'=${nj.mat2Tex(Result[0])}`\n\n\n\n\n\n\n\nThe corresponding elementary matrix E is such that\n\n\nCode\nopType === 'row' ? tex`A'=EA` : tex`A'=AE.`\n\n\n\n\n\n\n\nAs will be discussed later, we accumulate the E^{-1} from the operations to compute the basechange matrix.\n\n\nCode\ntex.block`E=${nj.mat2Tex(Result[1])}\\text{, and }E^{-1}=${nj.mat2Tex(Result[2])}`\n\n\n\n\n\n\n\nWith all the background definitions and notations at our disposal to present the reduction algorithm."
  },
  {
    "objectID": "tutorials/topology/smith/index.html#the-reduction-algorithm",
    "href": "tutorials/topology/smith/index.html#the-reduction-algorithm",
    "title": "Smith Normal Form",
    "section": "The Reduction Algorithm",
    "text": "The Reduction Algorithm\nBefore we discuss the algorithm in detail, feel free to take a quick detour to see the reduction in action!\nGiven an integer matrix A of any size, its reduction to the Smith normal form follows three major steps:\n\nStep 0: Set the offset\nThe algorithm relies on an offset parameter. Starting with offset=1, the reduction process works on the original matrix, inductively, by incrementing the offset. The offset determines how much of the matrix has been already been processed; it ranges from 1 to \\min\\{n,m\\}. When the function reduce(offset) is called, it assumes that the offset-block, block with the diagonal element (offset,offset) at its top-left corner, has to be processed, and all other rows and columns are already in good shape.\nTo get an idea of how offset plays its role, try choosing an offset from the slider. If the reduction algorithm is called it would assume that the green elements are already processed, and the (gray) offset-block is yet to be processed.\n\n\nCode\nviewof offset = Inputs.range([1, Math.min(n, m)], {\n  value: 1,\n  step: 1,\n  label: `offset`\n})\n\n\n\n\n\n\n\n\n\nCode\n{\n  const container = d3.create(\"div\").attr(\"class\", \"mat-container\");\n\n  container\n    .selectAll(\".row\")\n    .data(A)\n    .join(\"div\")\n    .attr(\"class\", \"row\")\n    .selectAll(\"div\")\n    .data((d) =&gt; d)\n    .join(\"div\")\n    .attr(\"class\", \"element\")\n    .text((d) =&gt; d);\n\n  container\n    .selectAll(\".row\")\n    .filter((d, k) =&gt; k &lt; offset - 1)\n    .selectAll(\".element\")\n    .style(\"background\", \"#BDF3C2\");\n  container\n    .selectAll(\".row\")\n    .selectAll(\".element\")\n    .filter((d, k) =&gt; k &lt; offset - 1)\n    .style(\"background\", \"#BDF3C2\");\n  yield container.node();\n}\n\n\n\n\n\n\n\nA Word of Caution: Note, however, that the green rows and columns may not be yet processed in the following demo matrix.\n\n\nStep 1: Find the Best Pivot\nFor a given offset, we find the best pivot: a (non-zero) element that divides all other elements of the offset-block. This step involves finding repeatedly a pivot, then improving it.\nStep 1.1 Find a Pivot and antiPivot:\nFor a given offset-block, a pivot is a non-zero element with the smallest absolute value; shown with a green border below.\nIf the pivot-block is has all zero entries, we return the step. If not, a pivot can be found in O(mn)-time.\n\n\nCode\n{\n  const container = d3.create(\"div\").attr(\"class\", \"mat-container\");\n\n  container\n    .selectAll(\".row\")\n    .data(A)\n    .join(\"div\")\n    .attr(\"class\", \"row\")\n    .selectAll(\"div\")\n    .data((d) =&gt; d)\n    .join(\"div\")\n    .attr(\"class\", \"element\")\n    .text((d) =&gt; d);\n\n  container\n    .selectAll(\".row\")\n    .filter((d, k) =&gt; k &lt; offset - 1)\n    .selectAll(\".element\")\n    .style(\"background\", \"#BDF3C2\");\n  container\n    .selectAll(\".row\")\n    .selectAll(\".element\")\n    .filter((d, k) =&gt; k &lt; offset - 1)\n    .style(\"background\", \"#BDF3C2\");\n\n  container\n    .selectAll(\".row\")\n    .filter((d, k) =&gt; k === pivot[0])\n    .selectAll(\".element\")\n    .filter((d, k) =&gt; k === pivot[1])\n    .style(\"border\", \"2px solid green\");\n\n  container\n    .selectAll(\".row\")\n    .filter((d, k) =&gt; k === antiPivot[0])\n    .selectAll(\".element\")\n    .filter((d, k) =&gt; k === antiPivot[1])\n    .style(\"border\", \"2px solid red\");\n  yield container.node();\n}\n\n\n\n\n\n\n\nWe say that pivot can still be improved if, in the offset-block, there is an antiPivot: an element that does not divisible by the current pivot. In the demo, an antiPivot of a pivot is shown with a red border.\nIf antiPivot does not exist, we skip Step 1.1. Otherwise, we improve the pivot.\nStep 1.2 Improve the Pivot\n\n\nCode\n{\n  if (antiPivot.length === 0)\n    return md`For the offset-block, we see that the pivot is already the best. So, we skip this step.`;\n  else\n    return md`For the offset-block, we see that the antiPivot exists. So, we take this step.`;\n}\n\n\n\n\n\n\n\nLet [i,j] and [s,t] be the positions of pivot and antiPivot.\nCase I (i=s): If the pivot and antiPivot are on the same row, replace the antiPivot column by q \\times the pivot column, where a_{it}= qa_{ij}+r with 0&lt;r&lt;a_{ij}. As a result, a_{it} becomes r after the operations, and a_{ij} fails to to the pivot for the output offset-block.\nCase II (j=t): same operation as in Case I, but for rows.\nCase III (i\\neq s, j\\neq t): We assume for this case that a_{ij} divides all entries (of the current offset-block) in its row and column. If not, we are back to Case I.\nUsing this assumption we can replace the antiPivot row by  the pivot row, where q=\\frac{a_{sj}}{a_{ij}}. After the operation, a_{sj} becomes zero. If we now add replace the i-th row by adding it to the s-th row, a_{ij} does not change, however the (i,t)-th becomes a_{it}+a_{st}, which is not divisible by a_{ij}, and we are back to Case I.\nEach of the above cases yields a smaller pivot. We go back to Step 1.2 until the best pivot is found.\n\n\nStep 2: Move Pivot\nOnce the pivot is improved, we move the pivot to the top-left corner of the offset block by at most two elementary operations involving exchanging rows / columns.\n\n\nStep 3: Diagonalize\nSince the pivot divides all other elements in the offset block, one can find the right multiplier q for each row below and each column to its right to make entries zero by a series of operations involving replacing rows and columns.\nWe then increment the offset, and move to step 1.\nResult:\n\n\nCode\ntex`D=${nj.mat2Tex(NF.D)}`\n\n\n\n\n\n\n\n\n\nCode\ntex.block`\nQ^{-1}=${nj.mat2Tex(NF.Qinv)}\\text{, and }\nP=${nj.mat2Tex(NF.P)}\n`\n\n\n\n\n\n\n\nOne can check that ."
  },
  {
    "objectID": "tutorials/topology/smith/index.html#change-of-bases",
    "href": "tutorials/topology/smith/index.html#change-of-bases",
    "title": "Smith Normal Form",
    "section": "Change of Bases",
    "text": "Change of Bases\nAs we know now, the algorithm works by pre or post multiplying the original matrix A by an elementary matrix at each step of the reduction. Let E_1,E_2,\\ldots,E_k and F_1,F_2,\\ldots,F_l be the elementary matrices corresponding to the row and column operations performed in the increaing order of the subscript. Then the final diagonal matrix D can be written as: D=E_k\\ldots E_2.E_1.A.F_1.F_2\\ldots F_l=Q^{-1}AP, where P=F_1.F_2\\ldots F_l and Q^{-1}=E_k\\ldots E_2.E_1.\nWe note that P and Q=E_1^{-1}E_2^{-1}\\ldots E_k^{-1} are the basechange matrices for \\mathbb Z^m (domain) and \\mathbb Z^n (co-domain), respectively.\n\n\nCode\ntex.block`\n\\mathcal{B}=${nj.vec2Tex(B)}\\text{, and }\n\\mathcal{B'}=${nj.vec2Tex(B1)}\n`\n\n\n\n\n\n\n\nLet \\mathcal{C},\\mathcal{C}' be the new bases. We compute:\n\n\nCode\ntex`\\mathcal{C}=${nj.vec2Tex(nj.changeBasis(B, NF.P))}`\n\n\n\n\n\n\n\n\n\nCode\ntex`\\mathcal{C}'=${nj.vec2Tex(nj.changeBasis(B1, NF.Q))}`"
  },
  {
    "objectID": "tutorials/topology/smith/index.html#discussion",
    "href": "tutorials/topology/smith/index.html#discussion",
    "title": "Smith Normal Form",
    "section": "Discussion",
    "text": "Discussion\nIf you are too excited to explore more on the subject, the reader is advised to call on [artin]. The examples in this tutorial are produced using codes from the JS package: tdajs/normal-form?. Visit the Github repo for more information. Happy coding!\n\n\nCode\nnj = require(\"https://bundle.run/@tdajs/normal-form@2.0.0\")\nA = {\n  let rows = n; //d3.randomInt(1, 10)();\n  let cols = m; //d3.randomInt(1, 10)();\n\n  return d3.range(0, rows).map((row) =&gt; {\n    return d3.range(0, cols).map((elm) =&gt; d3.randomInt(-5, 5)());\n  });\n}\nOperations = {\n  const obj = {\n    row: [\n      { name: \"exchangeRows\", args: [\"i\", \"j\"] },\n      { name: \"replaceRow\", args: [\"i\", \"j\", \"q\"] },\n      { name: \"multiplyRow\", args: [\"i\"] }\n    ],\n    col: [\n      { name: \"exchangeCols\", args: [\"i\", \"j\"] },\n      { name: \"replaceCol\", args: [\"i\", \"j\", \"q\"] },\n      { name: \"multiplyCol\", args: [\"i\"] }\n    ]\n  };\n  return obj;\n}\npivot = nj.findPivot(A, offset - 1)\nantiPivot = nj.findAntiPivot(pivot, A, offset - 1)\nResult = {\n  if (operation.includes(\"exchange\"))\n    return nj[operation](i - 1, j - 1, A, { copy: true });\n  else if (operation.includes(\"multiply\"))\n    return nj[operation](i - 1, q, A, { copy: true });\n  else return nj[operation](i - 1, j - 1, q, A, { copy: true });\n}\n\nNF = new nj.NormalForm(A)\nB = Array.from({ length: m }).map((e, i) =&gt; \"e_\" + i)\nB1 = Array.from({ length: n }).map((e, i) =&gt; \"e_\" + i + \"'\")"
  },
  {
    "objectID": "tutorials/topology/rips.html",
    "href": "tutorials/topology/rips.html",
    "title": "Vietoris–Rips Complex",
    "section": "",
    "text": "The Vietoris-Rips Complex is a special type of simplicial complex built on a metric space. This topological concept has been around for a long time, although its invigoration in the twenty-first century was by the TDA community. The popularity of the complex is due to its increasing use in shape reconstruction; see [1,3] for example. The easy (but not always efficient) algorithmic approaches to compute the Vietoris-Rips complex of a finite metric space makes it more palatable choice over its close relative: the Čech complexes.\n\nDefinition\nGiven a metric space (M,d_M) and a scale \\epsilon&gt;0, the Vietoris-Rips complex R_{\\epsilon}(M) is a simplicial complex such that \\sigma=\\{x_1,x_2,\\ldots,x_n\\}\\in R_\\epsilon(M) if and only if \\{x_1,x_2,\\ldots,x_n\\}\\subseteq M with diam(\\sigma)\\leq\\epsilon.\n\n\nDemo\nWe run a little JavaScript demo of Vietoris-Rips complex for a finite subset V under the Euclidean distance. Although the computed Rips complex is an abstract simplicial complex (without an embedding), we only show its shadow; see [2] for a definition.\n\ntex`\\epsilon=${scale}`\n\n\n\n\n\n\nAs we can see, the set V is currently empty, and the scale \\epsilon is set to zero. We pick a set of points in the plain by clicking on the canvas below. A word of caution: picking more than 30 points can substantially slow down your browser!\n\nV = [];\n\n\n\n\n\n\n\n{\n  const height = \"300px\";\n  const container = d3.create(\"div\").style(\"position\", \"relative\");\n  let svg = container\n    .append(\"svg\")\n    .attr(\"class\", \"canvas\")\n    .style(\"margin-left\", \"15px\")\n    .style(\"width\", \"90%\")\n    .style(\"height\", height)\n    .style(\"border\", \"0.5px solid #eee\");\n  \n  const triangles = svg.append(\"g\").attr(\"class\", \"triangles\");\n  const edges = svg.append(\"g\").attr(\"class\", \"edges\");\n  const vertices = svg.append(\"g\").attr(\"class\", \"vertices\");\n\n  // scale\n  container\n    .append(\"div\")\n    .style(\"width\", \"15px\")\n    .style(\"height\", height)\n    .style(\"background\", \"#eee\")\n    .style(\"position\", \"absolute\")\n    .style(\"top\", \"0\")\n    .style(\"bottom\", \"0\")\n    .append(\"div\")\n    .style(\"width\", \"100%\")\n    .style(\"height\", scale + \"px\")\n    .style(\"background\", \"steelblue\");\n  container\n    .append(\"div\")\n    .style(\"margin-left\", \"3px\")\n    .style(\"width\", height)\n    .style(\"display\", \"inline-block\")\n    .style(\"text-align\", \"center\")\n    .style(\"transform\", \"rotate(-90deg)\")\n    .style(\"transform-origin\", \"top left\")\n    .html(tex`\\epsilon`.outerHTML);\n\n  drawRips(svg, sc.rips(V, scale, 2));\n\n  svg.on(\"click\", (e) =&gt; {\n    const coord = d3.pointer(e);\n    V.push(coord);\n    drawRips(svg, sc.rips(V, scale, 2));\n  });\n  return container.node();\n}\n\n\n\n\n\n\nWe now use the slider to set the scale \\epsilon:\n\nviewof scale = Inputs.range([0, 300], {\n  step: 1,\n  value: 0,\n  label: tex`\\epsilon`\n})\n\n\n\n\n\n\n\nviewof btn = Inputs.button(\"clear\", {\n  value: null,\n  reduce: () =&gt; { V.length = 0; viewof scale.value = 0;viewof scale.dispatchEvent(new CustomEvent(\"input\")); }\n})\n\n\n\n\n\n\nAs we add more points or fiddle with the scale, the Betti numbers of the computed Vietoris-Rips complex can also be computed.\n\nimport { slider } from \"@jashkenas/inputs\"\n\n\n\n\n\n\n\nsc = require(\"https://cdn.jsdelivr.net/npm/@tdajs/simplicial-complex@1.2.1/dist/min.js\")\n\n\n\n\n\n\n\ndrawRips = function (svg, rips) {\n  if (rips.simplices[2]) {\n    svg.selectAll(\".triangle\")\n      .data(rips.simplices[2])\n      .join(\"path\")\n      .attr(\"class\", \"triangle\")\n      .attr(\"d\", (d) =&gt; d3.line()(d.map((v) =&gt; V[v])))\n      .attr(\"fill\", \"lightgreen\")\n      .attr(\"stroke\", \"none\")\n      .attr(\"opacity\", \"0.5\");\n  }\n  if (rips.simplices[1]) {\n    svg.selectAll(\".edge\")\n      .data(rips.simplices[1])\n      .join(\"path\")\n      .attr(\"class\", \"edge\")\n      .attr(\"d\", (d) =&gt; d3.line()(d.map((v) =&gt; V[v])))\n      .attr(\"stroke\", \"red\");\n  }\n\n  svg.selectAll(\".vertex\")\n    .data(V)\n    .join(\"circle\")\n    .attr(\"class\", \"vertex\")\n    .attr(\"class\", \"vertex\")\n    .attr(\"cx\", (d) =&gt; d[0])\n    .attr(\"cy\", (d) =&gt; d[1])\n    .attr(\"r\", \"2px\")\n    .on(\"mouseover\", function () {\n      d3.select(this).attr(\"fill\", \"orange\").attr(\"r\", \"5px\");\n    })\n    .on(\"mouseout\", function () {\n      d3.select(this).attr(\"fill\", \"black\").attr(\"r\", \"2px\");\n    });\n    return svg;\n}"
  },
  {
    "objectID": "tutorials/data-science/order-statistics.html",
    "href": "tutorials/data-science/order-statistics.html",
    "title": "Order Statistics",
    "section": "",
    "text": "In statistics, a one-value summary of a random sample is called a statistic. Mean, standard deviation, median, min, max of a sample are some of the commonly used statistics. While the computation of mean and standard deviation use the actual sample values—min, max, and median are computed using only their relative positions or order. When a sample is sorted in non-decreasing order, the first and the last positions are the min and the max, and the middle position is the median. The notion of order statistics generalizes such summaries of a sample.\nSince this is the most prevalent use case, we always consider a random sample X_1, X_2,\\ldots, X_n to be i.i.d. from a continuous random variable X following a common probability distribution \\mathbb F(x)=\\mathbb P(X\\leq x). The ordered sample is denoted by X_{(1)}\\leq X_{(2)}\\leq\\ldots\\leq X_{(n)}, and X_{(k)} is called the kth-order statistic for any 1\\leq k\\leq n.\nSo, the 1st and the nth order statistics are simply the sample min and smaple max, respectively. Moreover, X_{\\left(\\frac{n-1}{2}\\right)} is the sample median if n is odd.\nBefore exploring the sampling distributions of the order statistics in full generality, we start playing around with two very special order statistics—min and max."
  },
  {
    "objectID": "tutorials/data-science/order-statistics.html#experimenting-with-the-extrema",
    "href": "tutorials/data-science/order-statistics.html#experimenting-with-the-extrema",
    "title": "Order Statistics",
    "section": "Experimenting with the Extrema",
    "text": "Experimenting with the Extrema\nLet us consider a random sample of size n=10 from the uniform distribution over the interval [0,1]. The sampled data-points are shown in Figure 1, along with the min (in green) and max (in red).\n\nPlot.plot({\n  height: 70,\n  x: { domain: [0,1] } ,\n  y: { ticks: 0 },\n  marks: [\n    Plot.ruleY([0], { stroke: 'lightgray' }),\n    Plot.dot({ length: 10 }, {\n      y: Array.from({ length: 10 }).fill(0),\n      x: d3.sort(Array.from({ length: 10 }, d3.randomUniform()) ),\n      fill: (d, i) =&gt; {\n        if(i===0)\n          return 'green'\n        if(i===9)\n          return 'red'\n        else\n          return 'lightgray'\n      }\n    })\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 1: A random sample of size 10 from \\mathrm{unif}([0,1]). The red and blue points denote the min and the max, respectively.\n\n\n\n\nIn this random instance of the sample, the sample min (equivalently max) is not very far from 0 (equaivalently 1). We wonder if this is generally the case across random instances. It would definitely be counter-intuitive if it turns out that way. In our setup, each sample point X_i takes on a value in [0,1], without any unfair bias in favor of a particular point—moreover, X_i is oblivious to the other draws X_j for i\\neq j. So, it is most natural to think any point in the interval is equally-likely to be the min and max. In order to put this intuition to test, we take m=5 random samples. Each sample is drawn on a line as before, and the samples are stacked vertically in Figure 2.\n\nPlot.plot({\n  height: 250,\n  marginLeft: 50,\n  x: { label: null, domain: [0,1]},\n  y: { label: null, tickRotate: 30, line: true, grid: true },\n  marks: [\n    Plot.dot( rUnif(5, 10), {\n      x: 'X', \n      y: (d) =&gt; \"sample \" + d.sample ,\n      fill: (d) =&gt; {\n        if(d.k === 1)\n          return 'green'\n        if(d.k === 10)\n          return 'red'\n        else\n          return 'lightgray'\n      }\n    })\n  ]\n});\n\n\n\n\n\n\n\n\nFigure 2: Samples are drawn on the horizontal lines, the red minimums and maximums are again shown in green and red.\n\n\n\n\nAs it turns out, sample mean U has a tendency to remain to close 0 and sample max V has a tendency to remain to close 1! More specifically, for a sample of size n from uniform [0,1] we have E[U]=\\frac{1}{n+1}\\text{ and }E[V]=\\frac{n}{n+1}."
  },
  {
    "objectID": "tutorials/data-science/order-statistics.html#probability-distributions-of-the-extrema",
    "href": "tutorials/data-science/order-statistics.html#probability-distributions-of-the-extrema",
    "title": "Order Statistics",
    "section": "Probability Distributions of the Extrema",
    "text": "Probability Distributions of the Extrema\nLet us try to prove a little more general result. We compute now the density of the extrema U and V of a sample from a general probability distribution.\n\nTheorem 1 (Density Function of V) If X_1, X_2, \\ldots, X_n are independent random variables with the common CDF F and density f, then the density of V is f_V(v)=nf(v)[F(v)]^{n-1}.\n\n\nSolution. We first compute the CDF F_V(v) of V, then differentiate it to get the PDF. In order to compute F_V(v), we note that V\\leq v if and only if every X_i\\leq v. So, \\begin{align}\nF_V(v) &=\\mathbb P(V\\leq v) \\\\\n&=\\mathbb P(X_1\\leq v, X_2\\leq v, \\ldots, X_n\\leq v) \\\\\n&=\\mathbb P(X_1\\leq v)\\mathbb P(X_2\\leq v)\\ldots\\mathbb P(X_n\\leq v) \\\\\n&=[F(v)]^n.\n\\end{align} Differentiating we get the density \n\\begin{align}\nf_V(v) &=\\frac{d}{dv}[F(v)]^n \\\\\n&=n[F(v)]^{n-1}\\frac{dF(v)}{dv} \\\\\n&=n[F(v)]^{n-1}f(v).\n\\end{align}\n\n\n\nExercise 1 Prove that, under the conditions of Theorem 1, the density of U is f_U(u)=nf(u)[1-F(u)]^{n-1}."
  },
  {
    "objectID": "tutorials/data-science/order-statistics.html#the-order-statistics-of-uniform-01",
    "href": "tutorials/data-science/order-statistics.html#the-order-statistics-of-uniform-01",
    "title": "Order Statistics",
    "section": "The Order Statistics of Uniform [0,1]",
    "text": "The Order Statistics of Uniform [0,1]\nWe can now apply the density obtained in Theorem 2 to compute the expectations of the order statistics for a sample of size n from uniform [0,1]. In this case, \nf(x)=\\begin{cases}\n1,&0\\leq x\\leq 1 \\\\\n0,&\\text{ otherwise}\n\\end{cases}\n And, \nF(x)=\\begin{cases}\n0,&x&lt;0 \\\\\nx,&0\\leq x\\leq 1\\\\\n1,&x&gt;1\n\\end{cases}\n As a result, the density of the kth-order statistic is \nf_k(x)=\\begin{cases}\n\\frac{n!}{(k-1)!(n-k)!}x^{k-1}(1-x)^{n-k},& 0\\leq x\\leq 1 \\\\\n0,&\\text{ otherwise}\n\\end{cases}\n The expectation is \n\\begin{align}\nE[X_{(k)}] &=\\int_0^1x\\cdot\\frac{n!}{(k-1)!(n-k)!}x^{k-1}(1-x)^{n-k}dx \\\\\n&=\\frac{n!}{(k-1)!(n-k)!}\\int_0^1x^k(1-x)^{n-k}dx.\n\\end{align}\n The form of the integral is known as the Beta function. Using its relation to binomial coefficients, we can then write \n\\begin{align}\nE[X_{(k)}]\n&=\\frac{n!}{(k-1)!(n-k)!}\\int_0^1x^k(1-x)^{n-k}dx \\\\\n&=\\frac{n!}{(k-1)!(n-k)!}\\cdot\\frac{k!(n-k)!}{(k+n-k+1)!} \\\\\n&=\\frac{k}{n+1}.\n\\end{align}\n The typical positions of random draws (when sorted) are equally spaced over [0,1] as evident in the following plot.\n\nviewof n = slider({ \n          min: 0,\n          max: 200,\n          step: 1, \n          label: `n` \n});\nviewof m = slider({     min: 0,\n          max: 200,step: 1, label: tex`m` });\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd = rUnif(m, n);\nPlot.plot({\n  y: { grid: true },\n  marks: [\n  Plot.dot(d,{ x: 'X', y: 'sample', fill: 'k'})\n  //Plot.ruleX(Array.from({ length: n }, (d, k) =&gt; ({ k: k+1, x: (k+1)/(n+1) })),     //{ x: 'x', stroke: 'k' })\n  ]\n});"
  },
  {
    "objectID": "tutorials/data-science/conf-int.html",
    "href": "tutorials/data-science/conf-int.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "Code\nd3.randomNormal()();"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sushovan Majhi",
    "section": "",
    "text": "Google Scholar\n  \n  \n    \n     GitHub\n  \n  \n    \n     LinkedIn\n  \n\n      \nI am currently an assistant professor of Data Science at GWU.\nBefore that, I was a visiting assistant professor at GWU. Prior to GWU, I was a postdoc researcher and MIDS lecturer at the University of California, Berkeley.\nWelcome to my homepage. The site showcases my research and software projects, occasional tutorials, sporadic rants, and more.\nFind my CV here."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Sushovan Majhi",
    "section": "Education",
    "text": "Education\n\n\nDoctor of Philosophy in Mathematics\nTulane University, New Orleans, USA\n\n2020\n\n\n\nMaster of Science in Mathematics\nTata Institute of Fundamental Research, Bangalore, India\n\n2012\n\n\n\nBachelor of Science (Hons. in Mathematics)\nRamakrishna Mission Vidyamandira, Calcutta University, India\n\n2009"
  },
  {
    "objectID": "index.html#research",
    "href": "index.html#research",
    "title": "Sushovan Majhi",
    "section": "Research",
    "text": "Research\nThe theme of my research revolves around the mathematical foundations of data science. My research interests lie broadly in the following domains:\n\nApplied Algebraic Topology\n\nTopological Data Analysis\n\nComputational Geometry\n\nPattern and Shape Matching\nStatistical Finance\n\nTo know more about my research program, visit the research page."
  },
  {
    "objectID": "index.html#teaching",
    "href": "index.html#teaching",
    "title": "Sushovan Majhi",
    "section": "Teaching",
    "text": "Teaching\nMy teaching interests span a broad spectrum of fields—including foundations of data science, statistics, machine learning, computer science, topological data analysis. Here are some courses I have taught in recent years:\n\nIntroduction to statistics (undergraduate, Tulane University)\nStatistics for data science (graduate, UC Berkeley)\nTopological data analysis (graduate, NIT Sikkim, India)\nData mining (graduate, George Washington University)\nLinear Algebra for Data Science (graduate, GWU)\nAlgorithm design for Data Science (graduate, GWU)\n\nFind my teaching statement here."
  },
  {
    "objectID": "index.html#software-and-computing",
    "href": "index.html#software-and-computing",
    "title": "Sushovan Majhi",
    "section": "Software and Computing",
    "text": "Software and Computing\nI am a coding hobbyist. I enjoy solving online coding challenges. Although Java is my favorite programming language, I usually code in JavaScript, Python, and R. Some of them are listed here.\n\nShape Reconstruction\nTo complement my research, I implemented my topological reconstruction algorithm for planar metric graphs in this library. The library is written in JavaScript and made available to users as a web-app.\ntitle: ShapeReconstruction\nwebapp\nGithub"
  },
  {
    "objectID": "index.html#invited-talks-and-presentations",
    "href": "index.html#invited-talks-and-presentations",
    "title": "Sushovan Majhi",
    "section": "Invited Talks and Presentations",
    "text": "Invited Talks and Presentations\nI had been a big fan of Beamer for quite some time. Who wouldn’t be when it comes to presenting slides full of math symbols? Although the math looked fancy and the audience was happy, the \\LaTeX-based framework had also disappointed me quite often. I found the framework too restrictive to customize; my slides looked exactly like others’!\nFeatures, that were lacking in Beamer during the time I broke up with it, were shining in Reveal JS. Since then, I have been using it, customizing it, and relishing it. Although, I prefer to edit the source code for my slides in Quarto and output them in Reveal JS format.\nList of my talks and presentations:\n\n\n\n\nJun 14, 2024\n\nDemystifying Latschev’s Theorem for Manifold Reconstruction Symposium on Computational Geometry (SoCG), Athens, Greece\n\n\nAbstract\n\nTopological reconstruction of a manifold from a sample around it is a challenging computational problem, with varied applications in topological data analysis and manifold learning. Manifold structures appear frequently and naturally in many fields of science. Examples include Euclidean surfaces, phase spaces of dynamical systems, configuration spaces of robots, etc. Inferring the homotopy type of an unknown manifold from a set of finite (often noisy) observations constitutes the finite reconstruction problem. Latschev in his remarkable paper established the existence of a sufficiently small scale for the Vietoris–Rips complex of a dense sample to faithfully retain the topology of the manifold. The result is only qualitative, hence impractical for applications. We will discuss a recent development that provides the first quantitative result, along with a novel proof Latshev’s theorem.\n\n\n\n\nMar 28, 2024\n\nDemystifying Latschev’s Theorem for Manifold Reconstruction Montana State University\n\n\nAbstract\n\nTopological reconstruction of a manifold from a sample around it is a challenging computational problem, with varied applications in topological data analysis and manifold learning. Manifold structures appear frequently and naturally in many fields of science. Examples include Euclidean surfaces, phase spaces of dynamical systems, configuration spaces of robots, etc. Inferring the homotopy type of an unknown manifold from a set of finite (often noisy) observations constitutes the finite reconstruction problem. Latschev in his remarkable paper established the existence of a sufficiently small scale for the Vietoris–Rips complex of a dense sample to faithfully retain the topology of the manifold. The result is only qualitative, hence impractical for applications. We will discuss a recent development that provides the first quantitative result, along with a novel proof Latshev’s theorem.\n\n\n\n\nAug 23, 2023\n\nDemystifying Latschev’s Theorem for Manifold Reconstruction Applied Algebraic Topology Research Network (AATRN) Links:\n[YouTube] \n[Slides] \n\n\nAbstract\n\nTopological reconstruction of a manifold from a sample around it is a challenging computational problem, with varied applications in topological data analysis and manifold learning. Manifold structures appear frequently and naturally in many fields of science. Examples include Euclidean surfaces, phase spaces of dynamical systems, configuration spaces of robots, etc. Inferring the homotopy type of an unknown manifold from a set of finite (often noisy) observations constitutes the finite reconstruction problem. Latschev in his remarkable paper established the existence of a sufficiently small scale for the Vietoris–Rips complex of a dense sample to faithfully retain the topology of the manifold. The result is only qualitative, hence impractical for applications. We will discuss a recent development that provides the first quantitative result, along with a novel proof Latshev’s theorem.\n\n\n\n\nAug 3, 2023\n\nGraph Move’s Distance The 34th Canadian Conference on Computational Geometry Links:\n[url] \n\n\n\nOct 15, 2022\n\nSimilarity Measures for Geometric Graphs Fall Workshop on Computational Geometry, North Carolina State University Links:\n[url] \n\n\n\nJan 20, 2022\n\nA Taste of Topological Data Analysis (TDA): Reconstruction of Shapes ICFAI, Tripura Links:\n[url] \n\n\nAbstract\n\nTopological data analysis (TDA) is a growing field of study that helps address data analysis questions. TDA is deemed a better alternative to traditional statistical approaches when the data inherit a topological and geometric structure. Most of the modern technologies at our service rely on ‘geometric shapes’ in some way or the other. Be it the Google Maps showing you the fastest route to your destination or the 3D printer on your desk creating an exact replica of a relic—shapes are being repeatedly sampled, reconstructed, and compared by intelligent machines. In this talk, we will catch a glimpse of how some of the famous topological concepts—like persistent homology, Vietoris-Rips and Cech complexes, Nerve Lemma, etc—lend themselves well to the reconstruction of shapes from a noisy sample.\n\n\n\n\nSep 30, 2021\n\nA Taste of Topological Data Analysis (TDA): Reconstruction of Shapes Hunter College, New York Links:\n[url] \n\n\nAbstract\n\nTopological data analysis (TDA) is a growing field of study that helps address data analysis questions. TDA is deemed a better alternative to traditional statistical approaches when the data inherit a topological and geometric structure. Most of the modern technologies at our service rely on ‘geometric shapes’ in some way or the other. Be it the Google Maps showing you the fastest route to your destination or the 3D printer on your desk creating an exact replica of a relic—shapes are being repeatedly sampled, reconstructed, and compared by intelligent machines. In this talk, we will catch a glimpse of how some of the famous topological concepts—like persistent homology, Vietoris-Rips and Cech complexes, Nerve Lemma, etc—lend themselves well to the reconstruction of shapes from a noisy sample.\n\n\n\n\nJan 21, 2020\n\nShape Comparison and Gromov-Hausdorff Distance Tulane University Links:\n[url] \n\n\nAbstract\n\nThe Gromov-Hausdorff distance between any two metric spaces was first introduced by M. Gromov in the context of Riemannian manifolds. This distance measure has recently received an increasing attention from researchers in the field of topological data analysis. In applications, shapes are modeled as abstract metric spaces, and the Gromov-Hausdorff distance has been shown to provide a robust and natural framework for shape comparison. In this talk, we will introduce the notion and address the difficulties in computing the distance between two Euclidean point-clouds. In the light of our recent findings, we will also describe an O(n log n)-time approximation algorithm for Gromov-Hausdorff distance on the real line with an approximation factor of 5/4.\n\n\n\n\nAug 8, 2019\n\nShape Reconstruction Tulane University Links:\n[url] \n\n\nAbstract\n\nMost of the modern technologies at our service rely on ‘shapes’ in some way or other. Be it the Google Maps showing you the fastest route to your destination eluding a crash or the 3D printer on your desk creating an exact replica of a relic; shapes are being repeatedly sampled, reconstructed, and compared by intelligent machines. With the advent of modern sampling technologies, shape reconstruction and comparison techniques have matured profoundly over the last decade.\n\n\n\n\nDec 3, 2016\n\nMusic, Machine, and Mathematics Graduate Colloquium, Tulane University Links:\n[pdf] \n\n\n\nApr 16, 2016\n\nComputational Complexity Graduate Colloquium, Tulane University Links:\n[pdf] \n\n\n\nSep 8, 2015\n\nThe Mathematical Mechanic Graduate Colloquium, Tulane University Links:\n[pdf] \n\n\n\nNo matching items"
  }
]